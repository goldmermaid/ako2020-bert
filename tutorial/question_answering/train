#!/usr/bin/env python

from __future__ import print_function

import os
import json
import pickle
import sys
import traceback

import mxnet as mx
import gluonnlp as nlp
from gluonnlp.data import SQuAD
from qa import SQuADTransform, preprocess_dataset
from bert_qa_evaluate import download_qa_ckpt, get_F1_EM, predict, PredResult, BertForQALoss, BertForQA

np.random.seed(6)
random.seed(6)
mx.random.seed(6)


####################################################################################
# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a single channel of input data called 'training'. Since we run in
# File mode, the input files are copied to the directory specified here.
channel_name='training'
training_path = os.path.join(input_path, channel_name)

# The function to execute the training.
def train(batch_size=32, lr=5e-5, epochs=3):
    print('Starting the training.')
    try:

        # Load pretrained BERT, vocabulary, and tokenier
        bert, vocab = nlp.model.get_model(
                                        name="bert_12_768_12",
                                        dataset_name="book_corpus_wiki_en_uncased",
                                        pretrained=True,
                                        use_pooler=False,
                                        use_decoder=False,
                                        use_classifier=False)
        tokenizer = nlp.data.BERTTokenizer(vocab=vocab, lower=lower)
        

        # Load data
        train_data = SQuAD(segment, version='2.0')
        transform = SQuADTransform(tokenizer, 
                                   copy.copy(tokenizer),
                                   max_seq_length=384,
                                   max_query_length=64,
                                   doc_stride=128,
                                   is_pad=True, 
                                   is_training=True, 
                                   do_lookup=False)
        train_data_transform, _ = preprocess_dataset(train_data, transform)
        train_dataloader = mx.gluon.data.DataLoader(
            train_data_transform, 
            batchify_fn=batchify_fn,
            batch_size=batch_size, 
            num_workers=4, shuffle=True)


        # Load model to new defined net
        net = BertForQA(bert=bert)
        net.span_classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)
        ctx = mx.gpu(0)
        
        # Load complete BertForQA parameters
        ckpt = download_qa_ckpt()
        net.load_parameters(ckpt, ctx=ctx)
        net.hybridize(static_alloc=True)
        
        # Define Trainer and Loss
        optimizer_params = {'learning_rate': lr}
        trainer = mx.gluon.Trainer(net.collect_params(), optimizer,
                                   optimizer_params, update_on_kvstore=False)
        loss_function = BertForQALoss()
        loss_function.hybridize(static_alloc=True)

        
        # Start Training
        epoch_tic = time.time()
        total_num = 0
#         log_num = 0
        for epoch in range(epochs):
            step_loss = 0.0
#             tic = time.time()
            for batch_id, data in enumerate(train_dataloader):
#                 set new lr
#                 step_num = set_new_lr(step_num, batch_id)

                # forward and backward
                with mx.autograd.record():
                    _, inputs, token_types, valid_length, start_label, end_label = data

#                     log_num += len(inputs)
                    total_num += len(inputs)

                    out = net(inputs.astype('float32').as_in_context(ctx),
                              token_types.astype('float32').as_in_context(ctx),
                              valid_length.astype('float32').as_in_context(ctx))

                    ls = loss_function(out, [start_label.astype('float32').as_in_context(ctx),
                                             end_label.astype('float32').as_in_context(ctx)]).mean()

#                     if accumulate:
#                         ls = ls / accumulate
                ls.backward()
                # update
                if not accumulate or (batch_id + 1) % accumulate == 0:
                    trainer.allreduce_grads()
                    nlp.utils.clip_grad_global_norm(params, 1)
                    trainer.update(1)

                step_loss += ls.asscalar()

#                 # log
#                 if (batch_id + 1) % log_interval == 0:
#                     toc = time.time()
#                     log.info('Epoch: {}, Batch: {}/{}, Loss={:.4f}, lr={:.7f} Time cost={:.1f} Thoughput={:.2f} samples/s'  # pylint: disable=line-too-long
#                              .format(epoch, batch_id, len(train_dataloader),
#                                      step_loss / log_interval,
#                                      trainer.learning_rate, toc - tic, log_num/(toc - tic)))
#                     tic = time.time()
#                     step_loss = 0.0
#                     log_num = 0
        epoch_toc = time.time()
        log.info('Time cost={:.2f} s, Thoughput={:.2f} samples/s'.format(
                epoch_toc - epoch_tic, total_num/(epoch_toc - epoch_tic)))



        # save the model
        net.save_parameters(os.path.join(model_path, 'model.params'))
        with open(os.path.join(model_path, 'artifact.json'), 'w') as f:
            json.dump({'model_name': model_name}, f)
        print('Training complete.')
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
