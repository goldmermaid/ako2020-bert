{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hands-on: Training and deploying Question Answering with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-trained language representations have been shown to improve many downstream NLP tasks such as question answering, and natural language inference. Devlin, Jacob, et al proposed BERT [1] (Bidirectional Encoder Representations from Transformers), which fine-tunes deep bidirectional representations on a wide range of tasks with minimal task-specific parameters, and obtained state- of-the-art results.\n",
    "\n",
    "In this tutorial, we will focus on adapting the BERT model for the question answering task on the SQuAD dataset. Specifically, we will:\n",
    "\n",
    "- understanding how to pre-process the SQuAD dataset to leverage the learnt representation in BERT,\n",
    "- adapting the BERT model to the question answering task, and\n",
    "- loading a trained model to perform inference on the SQuAD dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagemaker configuration\n",
    "\n",
    "This notebook requires mxnet-cu101 >= 1.6.0b20191102, gluonnlp >= 0.8.1\n",
    "We can create a sagemaker notebook instance with the lifecycle configuration file: sagemaker-lifecycle.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# One time script\n",
    "# !bash sagemaker-lifecycle.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws-mxnet-cu101mkl                 1.6.0              \n",
      "keras-mxnet                        2.2.4.2            \n",
      "mxnet-cu101                        1.6.0              \n",
      "mxnet-model-server                 1.0.8              \n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/mxnet_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "gluonnlp                           0.9.2              \n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/mxnet_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep mxnet\n",
    "!pip list | grep gluonnlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Loading MXNet and GluonNLP\n",
    "\n",
    "We first import the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import copy\n",
    "import io\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import gluonnlp as nlp\n",
    "import mxnet as mx\n",
    "\n",
    "from gluonnlp.data import SQuAD\n",
    "from bert.data.qa import SQuADTransform, preprocess_dataset\n",
    "import bert_qa_evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inspecting the SQuAD Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Then we take a look at the Stanford Question Answering Dataset (SQuAD). The dataset can be downloaded using the `nlp.data.SQuAD` API. In this tutorial, we create a small dataset with 3 samples from the SQuAD dataset for demonstration purpose.\n",
    "\n",
    "The question answering task on the SQuAD dataset is setup the following way. For each sample in the dataset, a context is provided. The context is usually a long paragraph which contains lots of information. Then a question asked based on the context. The goal is to find the text span in the context that answers the question in the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.555133Z",
     "start_time": "2019-06-14T01:45:27.418706Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /home/ec2-user/SageMaker/datasets/squad/tmpbagvu_rv/dev-v1.1.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/squad/dev-v1.1.zip...\n",
      "Number of samples in the created dataset subsampled from SQuAD = 3\n"
     ]
    }
   ],
   "source": [
    "full_data = nlp.data.SQuAD(segment='dev', version='1.1')\n",
    "# loading a subset of the dev set of SQuAD\n",
    "num_target_samples = 3\n",
    "target_samples = [full_data[i] for i in range(num_target_samples)]\n",
    "dataset = mx.gluon.data.SimpleDataset(target_samples)\n",
    "print('Number of samples in the created dataset subsampled from SQuAD = %d'%len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " '56be4db0acb8001400a502ec',\n",
       " 'Which NFL team represented the AFC at Super Bowl 50?',\n",
       " 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.',\n",
       " ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],\n",
       " [177, 177, 177])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_samples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the above example is the structure of the SQuAD dataset. Here, the question index is 2, the context index is 3, the answer index is 4, and the answer position index is 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.560564Z",
     "start_time": "2019-06-14T01:45:27.557274Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "question_idx = 2\n",
    "context_idx = 3\n",
    "answer_idx = 4\n",
    "answer_pos_idx = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's take a look at a sample from the dataset. In this sample, the question is about the location of the game, with a description about the Super Bowl 50 game as the context. Note that three different answer spans are correct for this question, and they start from index 403, 355 and 355 in the context respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.567303Z",
     "start_time": "2019-06-14T01:45:27.562425Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context:\n",
      "\n",
      "Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "\n",
      "Question\n",
      "Where did Super Bowl 50 take place?\n",
      "\n",
      "Correct Answer Spans\n",
      "['Santa Clara, California', \"Levi's Stadium\", \"Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"]\n",
      "\n",
      "Answer Span Start Indices:\n",
      "[403, 355, 355]\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[2]\n",
    "print('\\nContext:\\n')\n",
    "print(sample[context_idx])\n",
    "print(\"\\nQuestion\")\n",
    "print(sample[question_idx])\n",
    "print(\"\\nCorrect Answer Spans\")\n",
    "print(sample[answer_idx])\n",
    "print(\"\\nAnswer Span Start Indices:\")\n",
    "print(sample[answer_pos_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Data Pre-processing for QA with BERT\n",
    "\n",
    "Recall that during BERT pre-training, it takes a sentence pair as the input, separated by the 'SEP' special token. For SQuAD, we can feed the context-question pair as the sentence pair input. To use BERT to predict the starting and ending span of the answer, we can add a classification layer for each token in the context texts, to predict if a token is the start or the end of the answer span. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:30:12.299493Z",
     "start_time": "2019-06-14T01:30:12.183419Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![qa](resources/qa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In the next few code blocks, we will work on pre-processing the samples in the SQuAD dataset in the desired format with these special separators. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Pre-trained BERT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "First, let's use the *get_model* API in GluonNLP to get the model definition for BERT, and the vocabulary used for the BERT model. Note that we discard the pooler and classifier layers used for the next sentence prediction task, as well as the decoder layers for the masked language model task during the BERT pre-training phase. These layers are not useful for predicting the starting and ending indices of the answer span.\n",
    "\n",
    "The list of pre-trained BERT models available in GluonNLP can be found [here](http://gluon-nlp.mxnet.io/model_zoo/bert/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.715444Z",
     "start_time": "2019-06-14T01:45:27.569118Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab file is not found. Downloading.\n",
      "Downloading /home/ec2-user/SageMaker/models/7148235391387426985/7148235391387426985_book_corpus_wiki_en_uncased-a6607397.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/vocab/book_corpus_wiki_en_uncased-a6607397.zip...\n"
     ]
    }
   ],
   "source": [
    "bert_model, vocab = nlp.model.get_model('bert_12_768_12',\n",
    "                                        dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                        use_classifier=False,\n",
    "                                        use_decoder=False,\n",
    "                                        use_pooler=False,\n",
    "                                        pretrained=False)\n",
    "with open('vocab.json', 'w') as f:\n",
    "    f.write(vocab.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Note that there are several special tokens in the vocabulary for BERT. In particular, the `[SEP]` token is used for separating the sentence pairs, and the `[CLS]` token is added at the beginning of the sentence pairs. They will be used to pre-process the SQuAD dataset later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.720137Z",
     "start_time": "2019-06-14T01:45:27.717192Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(size=30522, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The second step is to process the samples using the same tokenizer used for BERT, which is provided as the `BERTTokenizer` API in GluonNLP. Note that instead of word level and character level representation, BERT uses subwords to represent a word, separated `##`. \n",
    "\n",
    "In the following example, the word `suspending` is tokenized as two subwords (`suspend` and `##ing`), and `numerals` is tokenized as three subwords (`nu`, `##meral`, `##s`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.731724Z",
     "start_time": "2019-06-14T01:45:27.721690Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'temporarily',\n",
       " 'suspend',\n",
       " '##ing',\n",
       " 'the',\n",
       " 'tradition',\n",
       " 'of',\n",
       " 'naming',\n",
       " 'each',\n",
       " 'super',\n",
       " 'bowl',\n",
       " 'game',\n",
       " 'with',\n",
       " 'roman',\n",
       " 'nu',\n",
       " '##meral',\n",
       " '##s']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nlp.data.BERTTokenizer(vocab=vocab, lower=True)\n",
    "\n",
    "tokenizer(\"as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sentence Pair Composition\n",
    "\n",
    "With the tokenizer inplace, we are ready to process the question-context texts and compose sentence pairs. The functionality is available via the `SQuADTransform` API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.897684Z",
     "start_time": "2019-06-14T01:45:27.734029Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Transform dataset costs 0.10 seconds.\n"
     ]
    }
   ],
   "source": [
    "transform = SQuADTransform(tokenizer, is_pad=False, is_training=False, do_lookup=False)\n",
    "dev_data_transform, _ = preprocess_dataset(dataset, transform)\n",
    "logging.info('The number of examples after preprocessing:{}'.format(len(dev_data_transform)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's take a look at the sample after the transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.904353Z",
     "start_time": "2019-06-14T01:45:27.899992Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "segment type: \n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "text length: 168\n",
      "\n",
      "sentence pair: \n",
      "['[CLS]', 'where', 'did', 'super', 'bowl', '50', 'take', 'place', '?', '[SEP]', 'super', 'bowl', '50', 'was', 'an', 'american', 'football', 'game', 'to', 'determine', 'the', 'champion', 'of', 'the', 'national', 'football', 'league', '(', 'nfl', ')', 'for', 'the', '2015', 'season', '.', 'the', 'american', 'football', 'conference', '(', 'afc', ')', 'champion', 'denver', 'broncos', 'defeated', 'the', 'national', 'football', 'conference', '(', 'nfc', ')', 'champion', 'carolina', 'panthers', '24', '–', '10', 'to', 'earn', 'their', 'third', 'super', 'bowl', 'title', '.', 'the', 'game', 'was', 'played', 'on', 'february', '7', ',', '2016', ',', 'at', 'levi', \"'\", 's', 'stadium', 'in', 'the', 'san', 'francisco', 'bay', 'area', 'at', 'santa', 'clara', ',', 'california', '.', 'as', 'this', 'was', 'the', '50th', 'super', 'bowl', ',', 'the', 'league', 'emphasized', 'the', '\"', 'golden', 'anniversary', '\"', 'with', 'various', 'gold', '-', 'themed', 'initiatives', ',', 'as', 'well', 'as', 'temporarily', 'suspend', '##ing', 'the', 'tradition', 'of', 'naming', 'each', 'super', 'bowl', 'game', 'with', 'roman', 'nu', '##meral', '##s', '(', 'under', 'which', 'the', 'game', 'would', 'have', 'been', 'known', 'as', '\"', 'super', 'bowl', 'l', '\"', ')', ',', 'so', 'that', 'the', 'logo', 'could', 'prominently', 'feature', 'the', 'arabic', 'nu', '##meral', '##s', '50', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "sample = dev_data_transform[2]\n",
    "print('\\nsegment type: \\n' + str(sample[2]))\n",
    "print('\\ntext length: ' + str(sample[3]))\n",
    "print('\\nsentence pair: \\n' + str(sample[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vocabulary Lookup\n",
    "\n",
    "Finally, we convert the transformed texts to subword indices, which are used to contructor NDArrays as the inputs to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.910853Z",
     "start_time": "2019-06-14T01:45:27.906127Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2073, 2106, 3565, 4605, 2753, 2202, 2173, 1029, 3, 3565, 4605, 2753, 2001, 2019, 2137, 2374, 2208, 2000, 5646, 1996, 3410, 1997, 1996, 2120, 2374, 2223, 1006, 5088, 1007, 2005, 1996, 2325, 2161, 1012, 1996, 2137, 2374, 3034, 1006, 10511, 1007, 3410, 7573, 14169, 3249, 1996, 2120, 2374, 3034, 1006, 22309, 1007, 3410, 3792, 12915, 2484, 1516, 2184, 2000, 7796, 2037, 2353, 3565, 4605, 2516, 1012, 1996, 2208, 2001, 2209, 2006, 2337, 1021, 1010, 2355, 1010, 2012, 11902, 1005, 1055, 3346, 1999, 1996, 2624, 3799, 3016, 2181, 2012, 4203, 10254, 1010, 2662, 1012, 2004, 2023, 2001, 1996, 12951, 3565, 4605, 1010, 1996, 2223, 13155, 1996, 1000, 3585, 5315, 1000, 2007, 2536, 2751, 1011, 11773, 11107, 1010, 2004, 2092, 2004, 8184, 28324, 2075, 1996, 4535, 1997, 10324, 2169, 3565, 4605, 2208, 2007, 3142, 16371, 28990, 2015, 1006, 2104, 2029, 1996, 2208, 2052, 2031, 2042, 2124, 2004, 1000, 3565, 4605, 1048, 1000, 1007, 1010, 2061, 2008, 1996, 8154, 2071, 14500, 3444, 1996, 5640, 16371, 28990, 2015, 2753, 1012, 3]\n"
     ]
    }
   ],
   "source": [
    "def vocab_lookup(example_id, subwords, type_ids, length, start, end):\n",
    "    indices = vocab[subwords]\n",
    "    return example_id, indices, type_ids, length, start, end\n",
    "\n",
    "dev_data_transform = dev_data_transform.transform(vocab_lookup, lazy=False)\n",
    "print(dev_data_transform[2][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Definition\n",
    "\n",
    "After the data is processed, we can define the model that uses the representation produced by BERT for predicting the starting and ending positions of the answer span. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We download a BERT model trained on the SQuAD dataset, prepare the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:32.221383Z",
     "start_time": "2019-06-14T01:45:27.922825Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ./bert_qa-7eb11865.zip4bd0e039-7638-47ed-b4f0-a76918fcb001 from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/models/bert_qa-7eb11865.zip...\n",
      "Downloaded checkpoint to ./bert_qa-7eb11865.params\n"
     ]
    }
   ],
   "source": [
    "net = bert_qa_evaluate.BertForQA(bert_model)\n",
    "ctx = mx.gpu(0)\n",
    "ckpt = bert_qa_evaluate.download_qa_ckpt()\n",
    "net.load_parameters(ckpt, ctx=ctx)\n",
    "\n",
    "batch_size = 1\n",
    "dev_dataloader = mx.gluon.data.DataLoader(\n",
    "    dev_data_transform, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:32.575976Z",
     "start_time": "2019-06-14T01:45:32.223336Z"
    }
   },
   "outputs": [],
   "source": [
    "all_results = collections.defaultdict(list)\n",
    "\n",
    "total_num = 0\n",
    "for data in dev_dataloader:\n",
    "    example_ids, inputs, token_types, valid_length, _, _ = data\n",
    "    total_num += len(inputs)\n",
    "    batch_size = inputs.shape[0]\n",
    "    output = net(inputs.astype('float32').as_in_context(ctx),\n",
    "                               token_types.astype('float32').as_in_context(ctx),\n",
    "                               valid_length.astype('float32').as_in_context(ctx))\n",
    "    pred_start, pred_end = mx.nd.split(output, axis=2, num_outputs=2)\n",
    "    example_ids = example_ids.asnumpy().tolist()\n",
    "    pred_start = pred_start.reshape(batch_size, -1).asnumpy()\n",
    "    pred_end = pred_end.reshape(batch_size, -1).asnumpy()\n",
    "    \n",
    "    for example_id, start, end in zip(example_ids, pred_start, pred_end):\n",
    "        all_results[example_id].append(bert_qa_evaluate.PredResult(start=start, end=end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:32.623482Z",
     "start_time": "2019-06-14T01:45:32.578002Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "\n",
      "Question: which nfl team represented the afc at super bowl 50 ?\n",
      "\n",
      "Top predictions: \n",
      "99.51% \t Denver Broncos\n",
      "0.18% \t The American Football Conference (AFC) champion Denver Broncos\n",
      "0.11% \t Broncos\n",
      "\n",
      "\n",
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "\n",
      "Question: which nfl team represented the nfc at super bowl 50 ?\n",
      "\n",
      "Top predictions: \n",
      "80.04% \t Carolina Panthers\n",
      "16.39% \t Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers\n",
      "2.49% \t Denver Broncos\n",
      "\n",
      "\n",
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "\n",
      "Question: where did super bowl 50 take place ?\n",
      "\n",
      "Top predictions: \n",
      "27.06% \t Levi's Stadium\n",
      "26.29% \t Levi's Stadium in the San Francisco Bay Area at Santa Clara, California\n",
      "10.51% \t Levi's Stadium in the San Francisco Bay Area\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert_qa_evaluate.simple_predict(dataset, all_results, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "Now we can put all the pieces together, and start fine-tuning the model with a few epochs.\n",
    "\n",
    "The full training script is provided in ```finetune_squad.py```, with 20+ hyperparameters to be setted up (such as batch_size, debug, epochs=1, gpu, log_interval, lr, etc.). Let us firstly take a look of this training script before running the next line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gluonnlp:05:43:27 Namespace(accumulate=None, batch_size=4, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', debug=False, doc_stride=128, epochs=1, gpu=0, log_interval=50, lr=5e-05, max_answer_length=30, max_query_length=64, max_seq_length=384, model_parameters=None, n_best_size=20, null_score_diff_threshold=0.0, only_predict=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, sentencepiece=None, test_batch_size=24, uncased=True, version_2=False, warmup_ratio=0.1)\n",
      "INFO:gluonnlp:05:43:31 Loading train data...\n",
      "INFO:gluonnlp:05:43:32 Number of records in Train data:87599\n",
      "Done! Transform dataset costs 49.24 seconds.\n",
      "INFO:gluonnlp:05:44:22 The number of examples after preprocessing:88641\n",
      "INFO:gluonnlp:05:44:22 Start Training\n",
      "INFO:gluonnlp:05:44:28 Epoch: 0, Batch: 49/22161, Loss=5.8097, lr=0.0000011 Time cost=6.5 Thoughput=30.88 samples/s\n",
      "INFO:gluonnlp:05:44:34 Epoch: 0, Batch: 99/22161, Loss=5.1750, lr=0.0000023 Time cost=5.8 Thoughput=34.45 samples/s\n",
      "INFO:gluonnlp:05:44:40 Epoch: 0, Batch: 149/22161, Loss=4.7052, lr=0.0000034 Time cost=5.8 Thoughput=34.41 samples/s\n",
      "INFO:gluonnlp:05:44:46 Epoch: 0, Batch: 199/22161, Loss=4.2641, lr=0.0000045 Time cost=5.9 Thoughput=33.72 samples/s\n",
      "INFO:gluonnlp:05:44:52 Epoch: 0, Batch: 249/22161, Loss=3.8899, lr=0.0000056 Time cost=5.8 Thoughput=34.33 samples/s\n",
      "INFO:gluonnlp:05:44:58 Epoch: 0, Batch: 299/22161, Loss=3.7820, lr=0.0000068 Time cost=5.7 Thoughput=34.83 samples/s\n",
      "INFO:gluonnlp:05:45:03 Epoch: 0, Batch: 349/22161, Loss=3.6630, lr=0.0000079 Time cost=5.7 Thoughput=34.91 samples/s\n",
      "INFO:gluonnlp:05:45:09 Epoch: 0, Batch: 399/22161, Loss=3.1294, lr=0.0000090 Time cost=5.7 Thoughput=35.17 samples/s\n",
      "INFO:gluonnlp:05:45:15 Epoch: 0, Batch: 449/22161, Loss=2.9049, lr=0.0000102 Time cost=5.7 Thoughput=35.16 samples/s\n",
      "INFO:gluonnlp:05:45:20 Epoch: 0, Batch: 499/22161, Loss=2.8820, lr=0.0000113 Time cost=5.7 Thoughput=35.03 samples/s\n",
      "INFO:gluonnlp:05:45:26 Epoch: 0, Batch: 549/22161, Loss=2.7903, lr=0.0000124 Time cost=5.7 Thoughput=35.06 samples/s\n",
      "INFO:gluonnlp:05:45:32 Epoch: 0, Batch: 599/22161, Loss=2.6310, lr=0.0000135 Time cost=5.8 Thoughput=34.51 samples/s\n",
      "INFO:gluonnlp:05:45:38 Epoch: 0, Batch: 649/22161, Loss=2.5931, lr=0.0000147 Time cost=5.8 Thoughput=34.70 samples/s\n",
      "INFO:gluonnlp:05:45:43 Epoch: 0, Batch: 699/22161, Loss=2.4471, lr=0.0000158 Time cost=5.7 Thoughput=34.79 samples/s\n",
      "INFO:gluonnlp:05:45:49 Epoch: 0, Batch: 749/22161, Loss=2.3888, lr=0.0000169 Time cost=5.8 Thoughput=34.34 samples/s\n",
      "INFO:gluonnlp:05:45:55 Epoch: 0, Batch: 799/22161, Loss=2.3481, lr=0.0000181 Time cost=5.8 Thoughput=34.38 samples/s\n",
      "INFO:gluonnlp:05:46:01 Epoch: 0, Batch: 849/22161, Loss=2.1659, lr=0.0000192 Time cost=5.8 Thoughput=34.39 samples/s\n",
      "INFO:gluonnlp:05:46:07 Epoch: 0, Batch: 899/22161, Loss=2.2706, lr=0.0000203 Time cost=5.8 Thoughput=34.35 samples/s\n",
      "INFO:gluonnlp:05:46:12 Epoch: 0, Batch: 949/22161, Loss=2.1551, lr=0.0000214 Time cost=5.8 Thoughput=34.42 samples/s\n",
      "INFO:gluonnlp:05:46:18 Epoch: 0, Batch: 999/22161, Loss=1.9037, lr=0.0000226 Time cost=5.8 Thoughput=34.34 samples/s\n",
      "INFO:gluonnlp:05:46:25 Epoch: 0, Batch: 1049/22161, Loss=1.9672, lr=0.0000237 Time cost=6.4 Thoughput=31.17 samples/s\n",
      "INFO:gluonnlp:05:46:31 Epoch: 0, Batch: 1099/22161, Loss=1.8710, lr=0.0000248 Time cost=5.8 Thoughput=34.49 samples/s\n",
      "INFO:gluonnlp:05:46:36 Epoch: 0, Batch: 1149/22161, Loss=1.9098, lr=0.0000259 Time cost=5.7 Thoughput=34.86 samples/s\n",
      "INFO:gluonnlp:05:46:42 Epoch: 0, Batch: 1199/22161, Loss=1.9958, lr=0.0000271 Time cost=5.8 Thoughput=34.24 samples/s\n",
      "INFO:gluonnlp:05:46:48 Epoch: 0, Batch: 1249/22161, Loss=1.9395, lr=0.0000282 Time cost=5.7 Thoughput=34.82 samples/s\n",
      "INFO:gluonnlp:05:46:54 Epoch: 0, Batch: 1299/22161, Loss=2.1261, lr=0.0000293 Time cost=5.8 Thoughput=34.33 samples/s\n",
      "INFO:gluonnlp:05:47:00 Epoch: 0, Batch: 1349/22161, Loss=1.9494, lr=0.0000305 Time cost=5.8 Thoughput=34.31 samples/s\n",
      "INFO:gluonnlp:05:47:05 Epoch: 0, Batch: 1399/22161, Loss=1.8177, lr=0.0000316 Time cost=5.9 Thoughput=34.12 samples/s\n",
      "INFO:gluonnlp:05:47:11 Epoch: 0, Batch: 1449/22161, Loss=1.8208, lr=0.0000327 Time cost=5.7 Thoughput=34.82 samples/s\n",
      "INFO:gluonnlp:05:47:17 Epoch: 0, Batch: 1499/22161, Loss=1.8704, lr=0.0000338 Time cost=5.8 Thoughput=34.62 samples/s\n",
      "INFO:gluonnlp:05:47:23 Epoch: 0, Batch: 1549/22161, Loss=1.7094, lr=0.0000350 Time cost=5.8 Thoughput=34.40 samples/s\n",
      "INFO:gluonnlp:05:47:29 Epoch: 0, Batch: 1599/22161, Loss=1.7048, lr=0.0000361 Time cost=5.8 Thoughput=34.46 samples/s\n",
      "INFO:gluonnlp:05:47:34 Epoch: 0, Batch: 1649/22161, Loss=2.0065, lr=0.0000372 Time cost=5.8 Thoughput=34.48 samples/s\n",
      "INFO:gluonnlp:05:47:40 Epoch: 0, Batch: 1699/22161, Loss=1.7780, lr=0.0000384 Time cost=5.8 Thoughput=34.52 samples/s\n",
      "INFO:gluonnlp:05:47:46 Epoch: 0, Batch: 1749/22161, Loss=1.6948, lr=0.0000395 Time cost=5.9 Thoughput=33.66 samples/s\n",
      "INFO:gluonnlp:05:47:52 Epoch: 0, Batch: 1799/22161, Loss=1.9326, lr=0.0000406 Time cost=5.9 Thoughput=34.12 samples/s\n",
      "INFO:gluonnlp:05:47:58 Epoch: 0, Batch: 1849/22161, Loss=1.6270, lr=0.0000417 Time cost=5.9 Thoughput=34.02 samples/s\n",
      "INFO:gluonnlp:05:48:04 Epoch: 0, Batch: 1899/22161, Loss=1.6297, lr=0.0000429 Time cost=5.9 Thoughput=34.11 samples/s\n",
      "INFO:gluonnlp:05:48:09 Epoch: 0, Batch: 1949/22161, Loss=1.7411, lr=0.0000440 Time cost=5.8 Thoughput=34.59 samples/s\n",
      "INFO:gluonnlp:05:48:15 Epoch: 0, Batch: 1999/22161, Loss=1.6023, lr=0.0000451 Time cost=5.8 Thoughput=34.41 samples/s\n",
      "INFO:gluonnlp:05:48:21 Epoch: 0, Batch: 2049/22161, Loss=1.6812, lr=0.0000463 Time cost=5.8 Thoughput=34.26 samples/s\n",
      "INFO:gluonnlp:05:48:27 Epoch: 0, Batch: 2099/22161, Loss=1.6358, lr=0.0000474 Time cost=5.8 Thoughput=34.49 samples/s\n",
      "INFO:gluonnlp:05:48:33 Epoch: 0, Batch: 2149/22161, Loss=1.6090, lr=0.0000485 Time cost=5.8 Thoughput=34.38 samples/s\n",
      "INFO:gluonnlp:05:48:39 Epoch: 0, Batch: 2199/22161, Loss=1.6558, lr=0.0000496 Time cost=5.8 Thoughput=34.36 samples/s\n",
      "INFO:gluonnlp:05:48:44 Epoch: 0, Batch: 2249/22161, Loss=1.7689, lr=0.0000499 Time cost=5.8 Thoughput=34.49 samples/s\n",
      "INFO:gluonnlp:05:48:50 Epoch: 0, Batch: 2299/22161, Loss=1.8008, lr=0.0000498 Time cost=5.8 Thoughput=34.71 samples/s\n",
      "INFO:gluonnlp:05:48:56 Epoch: 0, Batch: 2349/22161, Loss=1.7138, lr=0.0000497 Time cost=5.8 Thoughput=34.57 samples/s\n",
      "INFO:gluonnlp:05:49:02 Epoch: 0, Batch: 2399/22161, Loss=1.6214, lr=0.0000495 Time cost=5.8 Thoughput=34.41 samples/s\n",
      "INFO:gluonnlp:05:49:07 Epoch: 0, Batch: 2449/22161, Loss=1.4814, lr=0.0000494 Time cost=5.8 Thoughput=34.61 samples/s\n",
      "INFO:gluonnlp:05:49:13 Epoch: 0, Batch: 2499/22161, Loss=1.8414, lr=0.0000493 Time cost=5.8 Thoughput=34.57 samples/s\n",
      "INFO:gluonnlp:05:49:19 Epoch: 0, Batch: 2549/22161, Loss=1.8755, lr=0.0000492 Time cost=5.9 Thoughput=34.07 samples/s\n",
      "INFO:gluonnlp:05:49:25 Epoch: 0, Batch: 2599/22161, Loss=1.9990, lr=0.0000490 Time cost=5.8 Thoughput=34.22 samples/s\n",
      "INFO:gluonnlp:05:49:31 Epoch: 0, Batch: 2649/22161, Loss=1.6823, lr=0.0000489 Time cost=5.8 Thoughput=34.32 samples/s\n",
      "INFO:gluonnlp:05:49:37 Epoch: 0, Batch: 2699/22161, Loss=1.6133, lr=0.0000488 Time cost=5.9 Thoughput=34.17 samples/s\n",
      "INFO:gluonnlp:05:49:42 Epoch: 0, Batch: 2749/22161, Loss=1.7950, lr=0.0000487 Time cost=5.8 Thoughput=34.22 samples/s\n",
      "INFO:gluonnlp:05:49:48 Epoch: 0, Batch: 2799/22161, Loss=1.6795, lr=0.0000485 Time cost=5.8 Thoughput=34.20 samples/s\n",
      "INFO:gluonnlp:05:49:54 Epoch: 0, Batch: 2849/22161, Loss=1.5803, lr=0.0000484 Time cost=5.8 Thoughput=34.24 samples/s\n",
      "INFO:gluonnlp:05:50:00 Epoch: 0, Batch: 2899/22161, Loss=1.5087, lr=0.0000483 Time cost=5.8 Thoughput=34.35 samples/s\n",
      "INFO:gluonnlp:05:50:06 Epoch: 0, Batch: 2949/22161, Loss=1.7205, lr=0.0000482 Time cost=5.9 Thoughput=34.18 samples/s\n",
      "INFO:gluonnlp:05:50:12 Epoch: 0, Batch: 2999/22161, Loss=1.4771, lr=0.0000480 Time cost=5.8 Thoughput=34.42 samples/s\n",
      "INFO:gluonnlp:05:50:17 Epoch: 0, Batch: 3049/22161, Loss=1.7461, lr=0.0000479 Time cost=5.8 Thoughput=34.72 samples/s\n",
      "INFO:gluonnlp:05:50:23 Epoch: 0, Batch: 3099/22161, Loss=1.5840, lr=0.0000478 Time cost=5.8 Thoughput=34.54 samples/s\n",
      "INFO:gluonnlp:05:50:29 Epoch: 0, Batch: 3149/22161, Loss=1.6955, lr=0.0000477 Time cost=5.8 Thoughput=34.23 samples/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gluonnlp:05:50:35 Epoch: 0, Batch: 3199/22161, Loss=1.5063, lr=0.0000475 Time cost=5.8 Thoughput=34.21 samples/s\n",
      "INFO:gluonnlp:05:50:41 Epoch: 0, Batch: 3249/22161, Loss=1.6394, lr=0.0000474 Time cost=5.9 Thoughput=33.85 samples/s\n",
      "INFO:gluonnlp:05:50:47 Epoch: 0, Batch: 3299/22161, Loss=1.4939, lr=0.0000473 Time cost=5.9 Thoughput=33.66 samples/s\n",
      "INFO:gluonnlp:05:50:53 Epoch: 0, Batch: 3349/22161, Loss=1.5644, lr=0.0000472 Time cost=5.8 Thoughput=34.57 samples/s\n",
      "INFO:gluonnlp:05:50:58 Epoch: 0, Batch: 3399/22161, Loss=1.4531, lr=0.0000470 Time cost=5.9 Thoughput=34.16 samples/s\n",
      "INFO:gluonnlp:05:51:04 Epoch: 0, Batch: 3449/22161, Loss=1.7608, lr=0.0000469 Time cost=5.8 Thoughput=34.67 samples/s\n",
      "INFO:gluonnlp:05:51:10 Epoch: 0, Batch: 3499/22161, Loss=1.7769, lr=0.0000468 Time cost=5.8 Thoughput=34.33 samples/s\n",
      "INFO:gluonnlp:05:51:16 Epoch: 0, Batch: 3549/22161, Loss=1.5787, lr=0.0000467 Time cost=5.8 Thoughput=34.52 samples/s\n",
      "INFO:gluonnlp:05:51:22 Epoch: 0, Batch: 3599/22161, Loss=1.5676, lr=0.0000465 Time cost=5.8 Thoughput=34.46 samples/s\n",
      "INFO:gluonnlp:05:51:27 Epoch: 0, Batch: 3649/22161, Loss=1.5627, lr=0.0000464 Time cost=5.9 Thoughput=33.89 samples/s\n",
      "INFO:gluonnlp:05:51:33 Epoch: 0, Batch: 3699/22161, Loss=1.7620, lr=0.0000463 Time cost=5.7 Thoughput=34.79 samples/s\n",
      "INFO:gluonnlp:05:51:39 Epoch: 0, Batch: 3749/22161, Loss=1.6022, lr=0.0000462 Time cost=5.9 Thoughput=34.13 samples/s\n",
      "INFO:gluonnlp:05:51:45 Epoch: 0, Batch: 3799/22161, Loss=1.6258, lr=0.0000460 Time cost=5.8 Thoughput=34.47 samples/s\n",
      "INFO:gluonnlp:05:51:51 Epoch: 0, Batch: 3849/22161, Loss=1.3697, lr=0.0000459 Time cost=5.7 Thoughput=34.81 samples/s\n",
      "INFO:gluonnlp:05:51:56 Epoch: 0, Batch: 3899/22161, Loss=1.2487, lr=0.0000458 Time cost=5.8 Thoughput=34.72 samples/s\n",
      "INFO:gluonnlp:05:52:02 Epoch: 0, Batch: 3949/22161, Loss=1.5539, lr=0.0000457 Time cost=5.8 Thoughput=34.41 samples/s\n",
      "INFO:gluonnlp:05:52:08 Epoch: 0, Batch: 3999/22161, Loss=1.6474, lr=0.0000455 Time cost=5.9 Thoughput=34.04 samples/s\n",
      "INFO:gluonnlp:05:52:14 Epoch: 0, Batch: 4049/22161, Loss=1.5397, lr=0.0000454 Time cost=5.8 Thoughput=34.69 samples/s\n",
      "INFO:gluonnlp:05:52:20 Epoch: 0, Batch: 4099/22161, Loss=1.5841, lr=0.0000453 Time cost=5.9 Thoughput=34.01 samples/s\n",
      "INFO:gluonnlp:05:52:26 Epoch: 0, Batch: 4149/22161, Loss=1.3713, lr=0.0000452 Time cost=5.9 Thoughput=34.09 samples/s\n",
      "INFO:gluonnlp:05:52:31 Epoch: 0, Batch: 4199/22161, Loss=1.5000, lr=0.0000450 Time cost=5.9 Thoughput=34.11 samples/s\n",
      "INFO:gluonnlp:05:52:37 Epoch: 0, Batch: 4249/22161, Loss=1.4740, lr=0.0000449 Time cost=5.8 Thoughput=34.37 samples/s\n",
      "INFO:gluonnlp:05:52:43 Epoch: 0, Batch: 4299/22161, Loss=1.4478, lr=0.0000448 Time cost=5.8 Thoughput=34.30 samples/s\n",
      "INFO:gluonnlp:05:52:49 Epoch: 0, Batch: 4349/22161, Loss=1.5544, lr=0.0000447 Time cost=5.8 Thoughput=34.28 samples/s\n",
      "INFO:gluonnlp:05:52:55 Epoch: 0, Batch: 4399/22161, Loss=1.2853, lr=0.0000445 Time cost=5.8 Thoughput=34.64 samples/s\n",
      "INFO:gluonnlp:05:53:01 Epoch: 0, Batch: 4449/22161, Loss=1.4969, lr=0.0000444 Time cost=5.8 Thoughput=34.57 samples/s\n",
      "INFO:gluonnlp:05:53:06 Epoch: 0, Batch: 4499/22161, Loss=1.6399, lr=0.0000443 Time cost=5.8 Thoughput=34.32 samples/s\n",
      "INFO:gluonnlp:05:53:12 Epoch: 0, Batch: 4549/22161, Loss=1.3405, lr=0.0000441 Time cost=5.9 Thoughput=33.90 samples/s\n",
      "INFO:gluonnlp:05:53:18 Epoch: 0, Batch: 4599/22161, Loss=1.5056, lr=0.0000440 Time cost=5.8 Thoughput=34.51 samples/s\n",
      "INFO:gluonnlp:05:53:24 Epoch: 0, Batch: 4649/22161, Loss=1.7638, lr=0.0000439 Time cost=5.8 Thoughput=34.32 samples/s\n",
      "INFO:gluonnlp:05:53:30 Epoch: 0, Batch: 4699/22161, Loss=1.3904, lr=0.0000438 Time cost=5.9 Thoughput=34.14 samples/s\n",
      "INFO:gluonnlp:05:53:36 Epoch: 0, Batch: 4749/22161, Loss=1.4739, lr=0.0000436 Time cost=5.9 Thoughput=34.16 samples/s\n",
      "INFO:gluonnlp:05:53:41 Epoch: 0, Batch: 4799/22161, Loss=1.3521, lr=0.0000435 Time cost=5.9 Thoughput=33.77 samples/s\n",
      "INFO:gluonnlp:05:53:47 Epoch: 0, Batch: 4849/22161, Loss=1.5834, lr=0.0000434 Time cost=6.0 Thoughput=33.49 samples/s\n",
      "INFO:gluonnlp:05:53:53 Epoch: 0, Batch: 4899/22161, Loss=1.3815, lr=0.0000433 Time cost=5.9 Thoughput=34.13 samples/s\n",
      "INFO:gluonnlp:05:53:59 Epoch: 0, Batch: 4949/22161, Loss=1.4490, lr=0.0000431 Time cost=5.8 Thoughput=34.36 samples/s\n",
      "INFO:gluonnlp:05:54:05 Epoch: 0, Batch: 4999/22161, Loss=1.4614, lr=0.0000430 Time cost=5.9 Thoughput=34.16 samples/s\n",
      "INFO:gluonnlp:05:54:11 Epoch: 0, Batch: 5049/22161, Loss=1.4999, lr=0.0000429 Time cost=5.8 Thoughput=34.36 samples/s\n",
      "INFO:gluonnlp:05:54:17 Epoch: 0, Batch: 5099/22161, Loss=1.5072, lr=0.0000428 Time cost=5.8 Thoughput=34.30 samples/s\n",
      "INFO:gluonnlp:05:54:22 Epoch: 0, Batch: 5149/22161, Loss=1.6129, lr=0.0000426 Time cost=5.8 Thoughput=34.24 samples/s\n",
      "INFO:gluonnlp:05:54:28 Epoch: 0, Batch: 5199/22161, Loss=1.2438, lr=0.0000425 Time cost=5.8 Thoughput=34.26 samples/s\n",
      "INFO:gluonnlp:05:54:34 Epoch: 0, Batch: 5249/22161, Loss=1.3251, lr=0.0000424 Time cost=5.8 Thoughput=34.42 samples/s\n",
      "INFO:gluonnlp:05:54:40 Epoch: 0, Batch: 5299/22161, Loss=1.3140, lr=0.0000423 Time cost=5.8 Thoughput=34.31 samples/s\n",
      "INFO:gluonnlp:05:54:46 Epoch: 0, Batch: 5349/22161, Loss=1.4898, lr=0.0000421 Time cost=5.9 Thoughput=33.81 samples/s\n",
      "INFO:gluonnlp:05:54:52 Epoch: 0, Batch: 5399/22161, Loss=1.4490, lr=0.0000420 Time cost=5.9 Thoughput=34.09 samples/s\n",
      "INFO:gluonnlp:05:54:58 Epoch: 0, Batch: 5449/22161, Loss=1.7018, lr=0.0000419 Time cost=5.8 Thoughput=34.45 samples/s\n",
      "INFO:gluonnlp:05:55:03 Epoch: 0, Batch: 5499/22161, Loss=1.4339, lr=0.0000418 Time cost=5.8 Thoughput=34.38 samples/s\n",
      "INFO:gluonnlp:05:55:09 Epoch: 0, Batch: 5549/22161, Loss=1.3329, lr=0.0000416 Time cost=5.8 Thoughput=34.47 samples/s\n",
      "INFO:gluonnlp:05:55:15 Epoch: 0, Batch: 5599/22161, Loss=1.4706, lr=0.0000415 Time cost=5.8 Thoughput=34.21 samples/s\n",
      "INFO:gluonnlp:05:55:21 Epoch: 0, Batch: 5649/22161, Loss=1.5433, lr=0.0000414 Time cost=5.8 Thoughput=34.68 samples/s\n",
      "INFO:gluonnlp:05:55:27 Epoch: 0, Batch: 5699/22161, Loss=1.3735, lr=0.0000413 Time cost=5.8 Thoughput=34.64 samples/s\n",
      "INFO:gluonnlp:05:55:32 Epoch: 0, Batch: 5749/22161, Loss=1.2324, lr=0.0000411 Time cost=5.9 Thoughput=34.11 samples/s\n",
      "INFO:gluonnlp:05:55:38 Epoch: 0, Batch: 5799/22161, Loss=1.4558, lr=0.0000410 Time cost=5.8 Thoughput=34.43 samples/s\n",
      "INFO:gluonnlp:05:55:44 Epoch: 0, Batch: 5849/22161, Loss=1.6134, lr=0.0000409 Time cost=5.8 Thoughput=34.36 samples/s\n",
      "INFO:gluonnlp:05:55:50 Epoch: 0, Batch: 5899/22161, Loss=1.2670, lr=0.0000408 Time cost=5.8 Thoughput=34.72 samples/s\n",
      "INFO:gluonnlp:05:55:56 Epoch: 0, Batch: 5949/22161, Loss=1.2835, lr=0.0000406 Time cost=5.9 Thoughput=33.84 samples/s\n",
      "INFO:gluonnlp:05:56:02 Epoch: 0, Batch: 5999/22161, Loss=1.4971, lr=0.0000405 Time cost=5.8 Thoughput=34.68 samples/s\n",
      "INFO:gluonnlp:05:56:07 Epoch: 0, Batch: 6049/22161, Loss=1.3322, lr=0.0000404 Time cost=5.8 Thoughput=34.53 samples/s\n",
      "INFO:gluonnlp:05:56:13 Epoch: 0, Batch: 6099/22161, Loss=1.3562, lr=0.0000403 Time cost=5.8 Thoughput=34.62 samples/s\n",
      "INFO:gluonnlp:05:56:19 Epoch: 0, Batch: 6149/22161, Loss=1.6235, lr=0.0000401 Time cost=5.9 Thoughput=33.86 samples/s\n",
      "INFO:gluonnlp:05:56:25 Epoch: 0, Batch: 6199/22161, Loss=1.5541, lr=0.0000400 Time cost=5.8 Thoughput=34.60 samples/s\n",
      "INFO:gluonnlp:05:56:31 Epoch: 0, Batch: 6249/22161, Loss=1.4109, lr=0.0000399 Time cost=5.8 Thoughput=34.44 samples/s\n",
      "INFO:gluonnlp:05:56:36 Epoch: 0, Batch: 6299/22161, Loss=1.4174, lr=0.0000398 Time cost=5.8 Thoughput=34.34 samples/s\n",
      "INFO:gluonnlp:05:56:42 Epoch: 0, Batch: 6349/22161, Loss=1.6550, lr=0.0000396 Time cost=5.8 Thoughput=34.60 samples/s\n",
      "INFO:gluonnlp:05:56:48 Epoch: 0, Batch: 6399/22161, Loss=1.4635, lr=0.0000395 Time cost=5.9 Thoughput=33.71 samples/s\n",
      "INFO:gluonnlp:05:56:54 Epoch: 0, Batch: 6449/22161, Loss=1.4917, lr=0.0000394 Time cost=5.9 Thoughput=34.10 samples/s\n",
      "INFO:gluonnlp:05:57:00 Epoch: 0, Batch: 6499/22161, Loss=1.4979, lr=0.0000393 Time cost=5.9 Thoughput=34.11 samples/s\n",
      "INFO:gluonnlp:05:57:06 Epoch: 0, Batch: 6549/22161, Loss=1.2860, lr=0.0000391 Time cost=5.8 Thoughput=34.30 samples/s\n",
      "INFO:gluonnlp:05:57:11 Epoch: 0, Batch: 6599/22161, Loss=1.4995, lr=0.0000390 Time cost=5.8 Thoughput=34.48 samples/s\n",
      "INFO:gluonnlp:05:57:17 Epoch: 0, Batch: 6649/22161, Loss=1.3677, lr=0.0000389 Time cost=5.8 Thoughput=34.23 samples/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gluonnlp:05:57:23 Epoch: 0, Batch: 6699/22161, Loss=1.2483, lr=0.0000388 Time cost=5.8 Thoughput=34.45 samples/s\n",
      "INFO:gluonnlp:05:57:29 Epoch: 0, Batch: 6749/22161, Loss=1.2119, lr=0.0000386 Time cost=5.8 Thoughput=34.60 samples/s\n",
      "INFO:gluonnlp:05:57:35 Epoch: 0, Batch: 6799/22161, Loss=1.3646, lr=0.0000385 Time cost=5.9 Thoughput=34.06 samples/s\n",
      "INFO:gluonnlp:05:57:41 Epoch: 0, Batch: 6849/22161, Loss=1.5034, lr=0.0000384 Time cost=5.7 Thoughput=34.84 samples/s\n",
      "INFO:gluonnlp:05:57:46 Epoch: 0, Batch: 6899/22161, Loss=1.3373, lr=0.0000383 Time cost=5.8 Thoughput=34.29 samples/s\n",
      "INFO:gluonnlp:05:57:52 Epoch: 0, Batch: 6949/22161, Loss=1.3457, lr=0.0000381 Time cost=5.9 Thoughput=34.15 samples/s\n",
      "INFO:gluonnlp:05:57:58 Epoch: 0, Batch: 6999/22161, Loss=1.4209, lr=0.0000380 Time cost=5.8 Thoughput=34.21 samples/s\n",
      "INFO:gluonnlp:05:58:04 Epoch: 0, Batch: 7049/22161, Loss=1.3403, lr=0.0000379 Time cost=5.8 Thoughput=34.38 samples/s\n",
      "INFO:gluonnlp:05:58:10 Epoch: 0, Batch: 7099/22161, Loss=1.4164, lr=0.0000378 Time cost=5.8 Thoughput=34.69 samples/s\n",
      "INFO:gluonnlp:05:58:15 Epoch: 0, Batch: 7149/22161, Loss=1.3257, lr=0.0000376 Time cost=5.9 Thoughput=34.11 samples/s\n",
      "INFO:gluonnlp:05:58:21 Epoch: 0, Batch: 7199/22161, Loss=1.5014, lr=0.0000375 Time cost=5.8 Thoughput=34.27 samples/s\n",
      "INFO:gluonnlp:05:58:27 Epoch: 0, Batch: 7249/22161, Loss=1.4091, lr=0.0000374 Time cost=5.8 Thoughput=34.36 samples/s\n",
      "INFO:gluonnlp:05:58:33 Epoch: 0, Batch: 7299/22161, Loss=1.2046, lr=0.0000373 Time cost=5.9 Thoughput=34.06 samples/s\n",
      "INFO:gluonnlp:05:58:39 Epoch: 0, Batch: 7349/22161, Loss=1.4434, lr=0.0000371 Time cost=5.9 Thoughput=33.99 samples/s\n",
      "INFO:gluonnlp:05:58:45 Epoch: 0, Batch: 7399/22161, Loss=1.3874, lr=0.0000370 Time cost=5.8 Thoughput=34.32 samples/s\n",
      "INFO:gluonnlp:05:58:51 Epoch: 0, Batch: 7449/22161, Loss=1.2798, lr=0.0000369 Time cost=5.9 Thoughput=34.06 samples/s\n",
      "INFO:gluonnlp:05:58:56 Epoch: 0, Batch: 7499/22161, Loss=1.2935, lr=0.0000368 Time cost=5.9 Thoughput=34.08 samples/s\n",
      "INFO:gluonnlp:05:59:02 Epoch: 0, Batch: 7549/22161, Loss=1.4538, lr=0.0000366 Time cost=5.9 Thoughput=34.03 samples/s\n",
      "INFO:gluonnlp:05:59:08 Epoch: 0, Batch: 7599/22161, Loss=1.3681, lr=0.0000365 Time cost=5.8 Thoughput=34.47 samples/s\n",
      "INFO:gluonnlp:05:59:14 Epoch: 0, Batch: 7649/22161, Loss=1.4447, lr=0.0000364 Time cost=5.8 Thoughput=34.31 samples/s\n",
      "INFO:gluonnlp:05:59:20 Epoch: 0, Batch: 7699/22161, Loss=1.1480, lr=0.0000363 Time cost=6.3 Thoughput=31.83 samples/s\n",
      "INFO:gluonnlp:05:59:26 Epoch: 0, Batch: 7749/22161, Loss=1.3168, lr=0.0000361 Time cost=5.9 Thoughput=33.96 samples/s\n",
      "INFO:gluonnlp:05:59:32 Epoch: 0, Batch: 7799/22161, Loss=1.2358, lr=0.0000360 Time cost=5.8 Thoughput=34.50 samples/s\n",
      "INFO:gluonnlp:05:59:38 Epoch: 0, Batch: 7849/22161, Loss=1.1247, lr=0.0000359 Time cost=5.8 Thoughput=34.61 samples/s\n",
      "INFO:gluonnlp:05:59:44 Epoch: 0, Batch: 7899/22161, Loss=1.3745, lr=0.0000358 Time cost=5.9 Thoughput=33.98 samples/s\n",
      "INFO:gluonnlp:05:59:50 Epoch: 0, Batch: 7949/22161, Loss=1.2606, lr=0.0000356 Time cost=6.0 Thoughput=33.46 samples/s\n",
      "INFO:gluonnlp:05:59:55 Epoch: 0, Batch: 7999/22161, Loss=1.3785, lr=0.0000355 Time cost=5.8 Thoughput=34.44 samples/s\n",
      "INFO:gluonnlp:06:00:01 Epoch: 0, Batch: 8049/22161, Loss=1.2677, lr=0.0000354 Time cost=5.8 Thoughput=34.37 samples/s\n",
      "INFO:gluonnlp:06:00:07 Epoch: 0, Batch: 8099/22161, Loss=1.4551, lr=0.0000352 Time cost=5.8 Thoughput=34.49 samples/s\n",
      "INFO:gluonnlp:06:00:13 Epoch: 0, Batch: 8149/22161, Loss=1.3311, lr=0.0000351 Time cost=5.8 Thoughput=34.63 samples/s\n",
      "INFO:gluonnlp:06:00:19 Epoch: 0, Batch: 8199/22161, Loss=1.4262, lr=0.0000350 Time cost=5.8 Thoughput=34.40 samples/s\n",
      "INFO:gluonnlp:06:00:24 Epoch: 0, Batch: 8249/22161, Loss=1.2632, lr=0.0000349 Time cost=5.8 Thoughput=34.21 samples/s\n",
      "INFO:gluonnlp:06:00:30 Epoch: 0, Batch: 8299/22161, Loss=1.2857, lr=0.0000347 Time cost=5.8 Thoughput=34.35 samples/s\n",
      "INFO:gluonnlp:06:00:36 Epoch: 0, Batch: 8349/22161, Loss=1.3150, lr=0.0000346 Time cost=5.9 Thoughput=34.16 samples/s\n",
      "INFO:gluonnlp:06:00:42 Epoch: 0, Batch: 8399/22161, Loss=1.3270, lr=0.0000345 Time cost=5.8 Thoughput=34.63 samples/s\n",
      "INFO:gluonnlp:06:00:48 Epoch: 0, Batch: 8449/22161, Loss=1.3634, lr=0.0000344 Time cost=5.8 Thoughput=34.50 samples/s\n",
      "INFO:gluonnlp:06:00:54 Epoch: 0, Batch: 8499/22161, Loss=1.3144, lr=0.0000342 Time cost=5.8 Thoughput=34.40 samples/s\n",
      "INFO:gluonnlp:06:00:59 Epoch: 0, Batch: 8549/22161, Loss=1.1504, lr=0.0000341 Time cost=5.9 Thoughput=34.13 samples/s\n",
      "INFO:gluonnlp:06:01:05 Epoch: 0, Batch: 8599/22161, Loss=1.3803, lr=0.0000340 Time cost=5.9 Thoughput=34.14 samples/s\n",
      "INFO:gluonnlp:06:01:11 Epoch: 0, Batch: 8649/22161, Loss=1.4154, lr=0.0000339 Time cost=5.9 Thoughput=34.12 samples/s\n",
      "INFO:gluonnlp:06:01:17 Epoch: 0, Batch: 8699/22161, Loss=1.3643, lr=0.0000337 Time cost=5.9 Thoughput=33.91 samples/s\n",
      "INFO:gluonnlp:06:01:23 Epoch: 0, Batch: 8749/22161, Loss=1.1469, lr=0.0000336 Time cost=5.9 Thoughput=34.03 samples/s\n",
      "INFO:gluonnlp:06:01:29 Epoch: 0, Batch: 8799/22161, Loss=1.4792, lr=0.0000335 Time cost=5.8 Thoughput=34.46 samples/s\n",
      "INFO:gluonnlp:06:01:35 Epoch: 0, Batch: 8849/22161, Loss=1.3448, lr=0.0000334 Time cost=5.9 Thoughput=33.87 samples/s\n",
      "INFO:gluonnlp:06:01:40 Epoch: 0, Batch: 8899/22161, Loss=1.3688, lr=0.0000332 Time cost=5.9 Thoughput=34.11 samples/s\n",
      "INFO:gluonnlp:06:01:46 Epoch: 0, Batch: 8949/22161, Loss=1.4033, lr=0.0000331 Time cost=5.9 Thoughput=33.90 samples/s\n",
      "INFO:gluonnlp:06:01:52 Epoch: 0, Batch: 8999/22161, Loss=1.3441, lr=0.0000330 Time cost=5.9 Thoughput=34.00 samples/s\n",
      "INFO:gluonnlp:06:01:58 Epoch: 0, Batch: 9049/22161, Loss=1.3854, lr=0.0000329 Time cost=5.8 Thoughput=34.26 samples/s\n",
      "INFO:gluonnlp:06:02:04 Epoch: 0, Batch: 9099/22161, Loss=1.2053, lr=0.0000327 Time cost=5.8 Thoughput=34.67 samples/s\n",
      "INFO:gluonnlp:06:02:10 Epoch: 0, Batch: 9149/22161, Loss=1.3886, lr=0.0000326 Time cost=5.8 Thoughput=34.49 samples/s\n",
      "INFO:gluonnlp:06:02:15 Epoch: 0, Batch: 9199/22161, Loss=1.1902, lr=0.0000325 Time cost=5.8 Thoughput=34.33 samples/s\n",
      "INFO:gluonnlp:06:02:21 Epoch: 0, Batch: 9249/22161, Loss=1.3558, lr=0.0000324 Time cost=5.9 Thoughput=34.02 samples/s\n",
      "INFO:gluonnlp:06:02:27 Epoch: 0, Batch: 9299/22161, Loss=1.3864, lr=0.0000322 Time cost=5.8 Thoughput=34.63 samples/s\n",
      "INFO:gluonnlp:06:02:33 Epoch: 0, Batch: 9349/22161, Loss=1.4472, lr=0.0000321 Time cost=5.8 Thoughput=34.65 samples/s\n",
      "INFO:gluonnlp:06:02:39 Epoch: 0, Batch: 9399/22161, Loss=1.2418, lr=0.0000320 Time cost=5.8 Thoughput=34.33 samples/s\n",
      "INFO:gluonnlp:06:02:45 Epoch: 0, Batch: 9449/22161, Loss=1.1443, lr=0.0000319 Time cost=5.9 Thoughput=33.91 samples/s\n",
      "INFO:gluonnlp:06:02:51 Epoch: 0, Batch: 9499/22161, Loss=1.1102, lr=0.0000317 Time cost=6.0 Thoughput=33.56 samples/s\n",
      "INFO:gluonnlp:06:02:56 Epoch: 0, Batch: 9549/22161, Loss=1.1490, lr=0.0000316 Time cost=5.9 Thoughput=33.86 samples/s\n",
      "INFO:gluonnlp:06:03:02 Epoch: 0, Batch: 9599/22161, Loss=1.3760, lr=0.0000315 Time cost=5.8 Thoughput=34.24 samples/s\n",
      "INFO:gluonnlp:06:03:08 Epoch: 0, Batch: 9649/22161, Loss=1.2648, lr=0.0000314 Time cost=5.8 Thoughput=34.64 samples/s\n",
      "INFO:gluonnlp:06:03:14 Epoch: 0, Batch: 9699/22161, Loss=1.2569, lr=0.0000312 Time cost=5.8 Thoughput=34.29 samples/s\n",
      "INFO:gluonnlp:06:03:20 Epoch: 0, Batch: 9749/22161, Loss=1.4231, lr=0.0000311 Time cost=5.8 Thoughput=34.69 samples/s\n",
      "INFO:gluonnlp:06:03:26 Epoch: 0, Batch: 9799/22161, Loss=1.2782, lr=0.0000310 Time cost=5.8 Thoughput=34.28 samples/s\n",
      "INFO:gluonnlp:06:03:31 Epoch: 0, Batch: 9849/22161, Loss=1.2394, lr=0.0000309 Time cost=5.8 Thoughput=34.56 samples/s\n",
      "INFO:gluonnlp:06:03:37 Epoch: 0, Batch: 9899/22161, Loss=1.1998, lr=0.0000307 Time cost=5.8 Thoughput=34.30 samples/s\n",
      "INFO:gluonnlp:06:03:43 Epoch: 0, Batch: 9949/22161, Loss=1.0828, lr=0.0000306 Time cost=5.9 Thoughput=33.80 samples/s\n",
      "INFO:gluonnlp:06:03:49 Epoch: 0, Batch: 9999/22161, Loss=1.1609, lr=0.0000305 Time cost=5.8 Thoughput=34.36 samples/s\n",
      "INFO:gluonnlp:06:03:55 Epoch: 0, Batch: 10049/22161, Loss=1.2391, lr=0.0000304 Time cost=5.8 Thoughput=34.21 samples/s\n",
      "INFO:gluonnlp:06:04:01 Epoch: 0, Batch: 10099/22161, Loss=1.3899, lr=0.0000302 Time cost=5.9 Thoughput=34.14 samples/s\n",
      "INFO:gluonnlp:06:04:06 Epoch: 0, Batch: 10149/22161, Loss=1.2097, lr=0.0000301 Time cost=5.8 Thoughput=34.33 samples/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gluonnlp:06:04:12 Epoch: 0, Batch: 10199/22161, Loss=1.1220, lr=0.0000300 Time cost=5.8 Thoughput=34.64 samples/s\n",
      "INFO:gluonnlp:06:04:18 Epoch: 0, Batch: 10249/22161, Loss=1.2764, lr=0.0000299 Time cost=5.8 Thoughput=34.33 samples/s\n",
      "INFO:gluonnlp:06:04:24 Epoch: 0, Batch: 10299/22161, Loss=1.3541, lr=0.0000297 Time cost=5.9 Thoughput=34.06 samples/s\n",
      "INFO:gluonnlp:06:04:30 Epoch: 0, Batch: 10349/22161, Loss=1.3388, lr=0.0000296 Time cost=5.8 Thoughput=34.55 samples/s\n",
      "INFO:gluonnlp:06:04:36 Epoch: 0, Batch: 10399/22161, Loss=1.1667, lr=0.0000295 Time cost=5.9 Thoughput=33.80 samples/s\n",
      "INFO:gluonnlp:06:04:41 Epoch: 0, Batch: 10449/22161, Loss=1.3826, lr=0.0000294 Time cost=5.8 Thoughput=34.59 samples/s\n",
      "INFO:gluonnlp:06:04:47 Epoch: 0, Batch: 10499/22161, Loss=1.3370, lr=0.0000292 Time cost=5.9 Thoughput=34.13 samples/s\n",
      "INFO:gluonnlp:06:04:53 Epoch: 0, Batch: 10549/22161, Loss=1.1427, lr=0.0000291 Time cost=5.9 Thoughput=33.83 samples/s\n",
      "INFO:gluonnlp:06:04:59 Epoch: 0, Batch: 10599/22161, Loss=1.2243, lr=0.0000290 Time cost=5.8 Thoughput=34.36 samples/s\n",
      "INFO:gluonnlp:06:05:05 Epoch: 0, Batch: 10649/22161, Loss=1.2650, lr=0.0000289 Time cost=5.9 Thoughput=33.76 samples/s\n",
      "INFO:gluonnlp:06:05:11 Epoch: 0, Batch: 10699/22161, Loss=1.1486, lr=0.0000287 Time cost=5.8 Thoughput=34.34 samples/s\n",
      "INFO:gluonnlp:06:05:17 Epoch: 0, Batch: 10749/22161, Loss=1.4202, lr=0.0000286 Time cost=5.9 Thoughput=33.66 samples/s\n",
      "INFO:gluonnlp:06:05:23 Epoch: 0, Batch: 10799/22161, Loss=1.1283, lr=0.0000285 Time cost=5.9 Thoughput=34.06 samples/s\n",
      "INFO:gluonnlp:06:05:28 Epoch: 0, Batch: 10849/22161, Loss=1.3442, lr=0.0000284 Time cost=5.8 Thoughput=34.38 samples/s\n",
      "INFO:gluonnlp:06:05:34 Epoch: 0, Batch: 10899/22161, Loss=1.2936, lr=0.0000282 Time cost=5.9 Thoughput=33.71 samples/s\n",
      "INFO:gluonnlp:06:05:40 Epoch: 0, Batch: 10949/22161, Loss=1.1877, lr=0.0000281 Time cost=5.9 Thoughput=34.02 samples/s\n",
      "INFO:gluonnlp:06:05:46 Epoch: 0, Batch: 10999/22161, Loss=1.3928, lr=0.0000280 Time cost=5.9 Thoughput=33.77 samples/s\n",
      "INFO:gluonnlp:06:05:52 Epoch: 0, Batch: 11049/22161, Loss=1.2700, lr=0.0000279 Time cost=5.9 Thoughput=34.04 samples/s\n",
      "INFO:gluonnlp:06:05:58 Epoch: 0, Batch: 11099/22161, Loss=1.1713, lr=0.0000277 Time cost=5.8 Thoughput=34.59 samples/s\n",
      "INFO:gluonnlp:06:06:04 Epoch: 0, Batch: 11149/22161, Loss=1.2748, lr=0.0000276 Time cost=5.8 Thoughput=34.51 samples/s\n",
      "INFO:gluonnlp:06:06:09 Epoch: 0, Batch: 11199/22161, Loss=1.2424, lr=0.0000275 Time cost=5.8 Thoughput=34.49 samples/s\n",
      "INFO:gluonnlp:06:06:15 Epoch: 0, Batch: 11249/22161, Loss=1.2208, lr=0.0000274 Time cost=5.8 Thoughput=34.46 samples/s\n",
      "INFO:gluonnlp:06:06:21 Epoch: 0, Batch: 11299/22161, Loss=1.1578, lr=0.0000272 Time cost=5.8 Thoughput=34.34 samples/s\n",
      "INFO:gluonnlp:06:06:27 Epoch: 0, Batch: 11349/22161, Loss=1.0691, lr=0.0000271 Time cost=5.8 Thoughput=34.52 samples/s\n",
      "INFO:gluonnlp:06:06:33 Epoch: 0, Batch: 11399/22161, Loss=1.2328, lr=0.0000270 Time cost=5.8 Thoughput=34.21 samples/s\n",
      "INFO:gluonnlp:06:06:38 Epoch: 0, Batch: 11449/22161, Loss=1.2093, lr=0.0000269 Time cost=5.9 Thoughput=33.99 samples/s\n",
      "INFO:gluonnlp:06:06:44 Epoch: 0, Batch: 11499/22161, Loss=1.2196, lr=0.0000267 Time cost=5.9 Thoughput=34.12 samples/s\n",
      "INFO:gluonnlp:06:06:50 Epoch: 0, Batch: 11549/22161, Loss=1.3900, lr=0.0000266 Time cost=5.9 Thoughput=33.89 samples/s\n",
      "INFO:gluonnlp:06:06:56 Epoch: 0, Batch: 11599/22161, Loss=1.1337, lr=0.0000265 Time cost=5.8 Thoughput=34.34 samples/s\n",
      "INFO:gluonnlp:06:07:02 Epoch: 0, Batch: 11649/22161, Loss=1.2441, lr=0.0000263 Time cost=5.8 Thoughput=34.38 samples/s\n",
      "INFO:gluonnlp:06:07:08 Epoch: 0, Batch: 11699/22161, Loss=1.2715, lr=0.0000262 Time cost=5.8 Thoughput=34.26 samples/s\n",
      "INFO:gluonnlp:06:07:14 Epoch: 0, Batch: 11749/22161, Loss=1.4333, lr=0.0000261 Time cost=5.8 Thoughput=34.58 samples/s\n",
      "INFO:gluonnlp:06:07:19 Epoch: 0, Batch: 11799/22161, Loss=1.2168, lr=0.0000260 Time cost=5.8 Thoughput=34.50 samples/s\n",
      "INFO:gluonnlp:06:07:25 Epoch: 0, Batch: 11849/22161, Loss=1.4028, lr=0.0000258 Time cost=5.8 Thoughput=34.63 samples/s\n",
      "INFO:gluonnlp:06:07:31 Epoch: 0, Batch: 11899/22161, Loss=1.3101, lr=0.0000257 Time cost=5.9 Thoughput=33.96 samples/s\n",
      "INFO:gluonnlp:06:07:37 Epoch: 0, Batch: 11949/22161, Loss=1.0145, lr=0.0000256 Time cost=5.9 Thoughput=34.11 samples/s\n",
      "INFO:gluonnlp:06:07:43 Epoch: 0, Batch: 11999/22161, Loss=1.3424, lr=0.0000255 Time cost=5.9 Thoughput=34.00 samples/s\n",
      "INFO:gluonnlp:06:07:49 Epoch: 0, Batch: 12049/22161, Loss=1.2765, lr=0.0000253 Time cost=5.9 Thoughput=34.15 samples/s\n",
      "INFO:gluonnlp:06:07:54 Epoch: 0, Batch: 12099/22161, Loss=1.1078, lr=0.0000252 Time cost=5.9 Thoughput=34.05 samples/s\n",
      "INFO:gluonnlp:06:08:00 Epoch: 0, Batch: 12149/22161, Loss=1.1834, lr=0.0000251 Time cost=5.7 Thoughput=34.84 samples/s\n",
      "INFO:gluonnlp:06:08:06 Epoch: 0, Batch: 12199/22161, Loss=1.0811, lr=0.0000250 Time cost=5.9 Thoughput=34.10 samples/s\n",
      "INFO:gluonnlp:06:08:12 Epoch: 0, Batch: 12249/22161, Loss=1.1593, lr=0.0000248 Time cost=5.8 Thoughput=34.47 samples/s\n",
      "INFO:gluonnlp:06:08:18 Epoch: 0, Batch: 12299/22161, Loss=1.1514, lr=0.0000247 Time cost=5.9 Thoughput=34.17 samples/s\n",
      "INFO:gluonnlp:06:08:24 Epoch: 0, Batch: 12349/22161, Loss=1.1345, lr=0.0000246 Time cost=5.8 Thoughput=34.24 samples/s\n",
      "INFO:gluonnlp:06:08:29 Epoch: 0, Batch: 12399/22161, Loss=1.3013, lr=0.0000245 Time cost=5.8 Thoughput=34.44 samples/s\n",
      "INFO:gluonnlp:06:08:35 Epoch: 0, Batch: 12449/22161, Loss=1.2342, lr=0.0000243 Time cost=5.8 Thoughput=34.50 samples/s\n",
      "INFO:gluonnlp:06:08:41 Epoch: 0, Batch: 12499/22161, Loss=1.2074, lr=0.0000242 Time cost=5.9 Thoughput=34.16 samples/s\n",
      "INFO:gluonnlp:06:08:47 Epoch: 0, Batch: 12549/22161, Loss=1.1023, lr=0.0000241 Time cost=5.9 Thoughput=33.83 samples/s\n",
      "INFO:gluonnlp:06:08:53 Epoch: 0, Batch: 12599/22161, Loss=1.1940, lr=0.0000240 Time cost=5.8 Thoughput=34.20 samples/s\n",
      "INFO:gluonnlp:06:08:59 Epoch: 0, Batch: 12649/22161, Loss=1.3584, lr=0.0000238 Time cost=5.9 Thoughput=34.08 samples/s\n",
      "INFO:gluonnlp:06:09:04 Epoch: 0, Batch: 12699/22161, Loss=1.3358, lr=0.0000237 Time cost=5.8 Thoughput=34.40 samples/s\n",
      "INFO:gluonnlp:06:09:10 Epoch: 0, Batch: 12749/22161, Loss=1.2695, lr=0.0000236 Time cost=5.8 Thoughput=34.61 samples/s\n",
      "INFO:gluonnlp:06:09:16 Epoch: 0, Batch: 12799/22161, Loss=1.3001, lr=0.0000235 Time cost=5.8 Thoughput=34.48 samples/s\n",
      "INFO:gluonnlp:06:09:22 Epoch: 0, Batch: 12849/22161, Loss=1.2996, lr=0.0000233 Time cost=5.8 Thoughput=34.38 samples/s\n",
      "INFO:gluonnlp:06:09:28 Epoch: 0, Batch: 12899/22161, Loss=1.0578, lr=0.0000232 Time cost=5.8 Thoughput=34.63 samples/s\n",
      "INFO:gluonnlp:06:09:34 Epoch: 0, Batch: 12949/22161, Loss=1.3608, lr=0.0000231 Time cost=5.9 Thoughput=33.97 samples/s\n",
      "INFO:gluonnlp:06:09:39 Epoch: 0, Batch: 12999/22161, Loss=1.3776, lr=0.0000230 Time cost=5.9 Thoughput=33.91 samples/s\n",
      "INFO:gluonnlp:06:09:45 Epoch: 0, Batch: 13049/22161, Loss=1.2194, lr=0.0000228 Time cost=5.8 Thoughput=34.55 samples/s\n",
      "INFO:gluonnlp:06:09:51 Epoch: 0, Batch: 13099/22161, Loss=1.3010, lr=0.0000227 Time cost=5.7 Thoughput=34.82 samples/s\n",
      "INFO:gluonnlp:06:09:57 Epoch: 0, Batch: 13149/22161, Loss=1.1439, lr=0.0000226 Time cost=5.9 Thoughput=34.06 samples/s\n",
      "INFO:gluonnlp:06:10:03 Epoch: 0, Batch: 13199/22161, Loss=1.0990, lr=0.0000225 Time cost=5.8 Thoughput=34.30 samples/s\n",
      "INFO:gluonnlp:06:10:09 Epoch: 0, Batch: 13249/22161, Loss=1.3393, lr=0.0000223 Time cost=5.9 Thoughput=34.08 samples/s\n",
      "INFO:gluonnlp:06:10:14 Epoch: 0, Batch: 13299/22161, Loss=1.0033, lr=0.0000222 Time cost=5.8 Thoughput=34.58 samples/s\n",
      "INFO:gluonnlp:06:10:20 Epoch: 0, Batch: 13349/22161, Loss=1.1879, lr=0.0000221 Time cost=5.8 Thoughput=34.23 samples/s\n",
      "INFO:gluonnlp:06:10:26 Epoch: 0, Batch: 13399/22161, Loss=1.0847, lr=0.0000220 Time cost=5.9 Thoughput=34.14 samples/s\n",
      "INFO:gluonnlp:06:10:32 Epoch: 0, Batch: 13449/22161, Loss=1.1724, lr=0.0000218 Time cost=5.8 Thoughput=34.27 samples/s\n",
      "INFO:gluonnlp:06:10:38 Epoch: 0, Batch: 13499/22161, Loss=0.8703, lr=0.0000217 Time cost=5.9 Thoughput=33.99 samples/s\n",
      "INFO:gluonnlp:06:10:44 Epoch: 0, Batch: 13549/22161, Loss=1.1591, lr=0.0000216 Time cost=5.9 Thoughput=34.07 samples/s\n",
      "INFO:gluonnlp:06:10:49 Epoch: 0, Batch: 13599/22161, Loss=1.2016, lr=0.0000215 Time cost=5.9 Thoughput=34.06 samples/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gluonnlp:06:10:55 Epoch: 0, Batch: 13649/22161, Loss=1.2725, lr=0.0000213 Time cost=5.9 Thoughput=34.03 samples/s\n",
      "INFO:gluonnlp:06:11:01 Epoch: 0, Batch: 13699/22161, Loss=1.2749, lr=0.0000212 Time cost=5.8 Thoughput=34.47 samples/s\n",
      "INFO:gluonnlp:06:11:07 Epoch: 0, Batch: 13749/22161, Loss=1.0370, lr=0.0000211 Time cost=5.8 Thoughput=34.46 samples/s\n",
      "INFO:gluonnlp:06:11:13 Epoch: 0, Batch: 13799/22161, Loss=1.1329, lr=0.0000210 Time cost=5.8 Thoughput=34.33 samples/s\n",
      "INFO:gluonnlp:06:11:19 Epoch: 0, Batch: 13849/22161, Loss=1.2716, lr=0.0000208 Time cost=5.8 Thoughput=34.54 samples/s\n",
      "INFO:gluonnlp:06:11:24 Epoch: 0, Batch: 13899/22161, Loss=1.2175, lr=0.0000207 Time cost=5.8 Thoughput=34.41 samples/s\n",
      "INFO:gluonnlp:06:11:30 Epoch: 0, Batch: 13949/22161, Loss=1.2390, lr=0.0000206 Time cost=5.9 Thoughput=34.10 samples/s\n",
      "INFO:gluonnlp:06:11:36 Epoch: 0, Batch: 13999/22161, Loss=0.9471, lr=0.0000205 Time cost=5.8 Thoughput=34.57 samples/s\n",
      "INFO:gluonnlp:06:11:42 Epoch: 0, Batch: 14049/22161, Loss=0.8729, lr=0.0000203 Time cost=5.8 Thoughput=34.29 samples/s\n",
      "INFO:gluonnlp:06:11:48 Epoch: 0, Batch: 14099/22161, Loss=1.3037, lr=0.0000202 Time cost=5.9 Thoughput=33.77 samples/s\n",
      "INFO:gluonnlp:06:11:54 Epoch: 0, Batch: 14149/22161, Loss=1.1952, lr=0.0000201 Time cost=5.8 Thoughput=34.38 samples/s\n",
      "INFO:gluonnlp:06:11:59 Epoch: 0, Batch: 14199/22161, Loss=1.2755, lr=0.0000200 Time cost=5.8 Thoughput=34.29 samples/s\n",
      "INFO:gluonnlp:06:12:05 Epoch: 0, Batch: 14249/22161, Loss=1.2366, lr=0.0000198 Time cost=5.9 Thoughput=33.77 samples/s\n",
      "INFO:gluonnlp:06:12:11 Epoch: 0, Batch: 14299/22161, Loss=1.1333, lr=0.0000197 Time cost=5.9 Thoughput=34.09 samples/s\n",
      "INFO:gluonnlp:06:12:18 Epoch: 0, Batch: 14349/22161, Loss=1.2005, lr=0.0000196 Time cost=6.3 Thoughput=31.91 samples/s\n",
      "INFO:gluonnlp:06:12:23 Epoch: 0, Batch: 14399/22161, Loss=1.0867, lr=0.0000195 Time cost=5.8 Thoughput=34.29 samples/s\n",
      "INFO:gluonnlp:06:12:29 Epoch: 0, Batch: 14449/22161, Loss=1.1446, lr=0.0000193 Time cost=5.8 Thoughput=34.66 samples/s\n",
      "INFO:gluonnlp:06:12:35 Epoch: 0, Batch: 14499/22161, Loss=1.4176, lr=0.0000192 Time cost=5.8 Thoughput=34.40 samples/s\n",
      "INFO:gluonnlp:06:12:41 Epoch: 0, Batch: 14549/22161, Loss=1.1036, lr=0.0000191 Time cost=5.9 Thoughput=34.04 samples/s\n",
      "INFO:gluonnlp:06:12:47 Epoch: 0, Batch: 14599/22161, Loss=1.2042, lr=0.0000190 Time cost=5.8 Thoughput=34.45 samples/s\n",
      "INFO:gluonnlp:06:12:52 Epoch: 0, Batch: 14649/22161, Loss=1.0723, lr=0.0000188 Time cost=5.8 Thoughput=34.38 samples/s\n",
      "INFO:gluonnlp:06:12:58 Epoch: 0, Batch: 14699/22161, Loss=1.1767, lr=0.0000187 Time cost=5.9 Thoughput=33.84 samples/s\n",
      "INFO:gluonnlp:06:13:04 Epoch: 0, Batch: 14749/22161, Loss=1.1395, lr=0.0000186 Time cost=5.9 Thoughput=34.15 samples/s\n",
      "INFO:gluonnlp:06:13:10 Epoch: 0, Batch: 14799/22161, Loss=1.1236, lr=0.0000185 Time cost=5.8 Thoughput=34.32 samples/s\n",
      "INFO:gluonnlp:06:13:16 Epoch: 0, Batch: 14849/22161, Loss=1.3549, lr=0.0000183 Time cost=5.9 Thoughput=33.98 samples/s\n",
      "INFO:gluonnlp:06:13:22 Epoch: 0, Batch: 14899/22161, Loss=1.1546, lr=0.0000182 Time cost=5.8 Thoughput=34.41 samples/s\n",
      "INFO:gluonnlp:06:13:28 Epoch: 0, Batch: 14949/22161, Loss=1.3358, lr=0.0000181 Time cost=5.8 Thoughput=34.53 samples/s\n",
      "INFO:gluonnlp:06:13:33 Epoch: 0, Batch: 14999/22161, Loss=1.2153, lr=0.0000180 Time cost=5.8 Thoughput=34.29 samples/s\n",
      "INFO:gluonnlp:06:13:39 Epoch: 0, Batch: 15049/22161, Loss=1.1109, lr=0.0000178 Time cost=5.9 Thoughput=33.85 samples/s\n",
      "INFO:gluonnlp:06:13:45 Epoch: 0, Batch: 15099/22161, Loss=1.0874, lr=0.0000177 Time cost=5.8 Thoughput=34.62 samples/s\n",
      "INFO:gluonnlp:06:13:51 Epoch: 0, Batch: 15149/22161, Loss=1.1828, lr=0.0000176 Time cost=5.9 Thoughput=34.09 samples/s\n",
      "INFO:gluonnlp:06:13:57 Epoch: 0, Batch: 15199/22161, Loss=0.9540, lr=0.0000174 Time cost=5.8 Thoughput=34.44 samples/s\n",
      "INFO:gluonnlp:06:14:03 Epoch: 0, Batch: 15249/22161, Loss=1.1767, lr=0.0000173 Time cost=5.8 Thoughput=34.34 samples/s\n",
      "INFO:gluonnlp:06:14:08 Epoch: 0, Batch: 15299/22161, Loss=1.2105, lr=0.0000172 Time cost=5.9 Thoughput=34.01 samples/s\n",
      "INFO:gluonnlp:06:14:14 Epoch: 0, Batch: 15349/22161, Loss=1.1827, lr=0.0000171 Time cost=5.8 Thoughput=34.52 samples/s\n",
      "INFO:gluonnlp:06:14:20 Epoch: 0, Batch: 15399/22161, Loss=1.0495, lr=0.0000169 Time cost=5.8 Thoughput=34.41 samples/s\n",
      "INFO:gluonnlp:06:14:26 Epoch: 0, Batch: 15449/22161, Loss=1.1646, lr=0.0000168 Time cost=5.8 Thoughput=34.41 samples/s\n",
      "INFO:gluonnlp:06:14:32 Epoch: 0, Batch: 15499/22161, Loss=1.1644, lr=0.0000167 Time cost=5.8 Thoughput=34.38 samples/s\n",
      "INFO:gluonnlp:06:14:37 Epoch: 0, Batch: 15549/22161, Loss=1.0703, lr=0.0000166 Time cost=5.8 Thoughput=34.60 samples/s\n",
      "INFO:gluonnlp:06:14:43 Epoch: 0, Batch: 15599/22161, Loss=1.3473, lr=0.0000164 Time cost=5.9 Thoughput=34.04 samples/s\n",
      "INFO:gluonnlp:06:14:49 Epoch: 0, Batch: 15649/22161, Loss=1.1082, lr=0.0000163 Time cost=6.0 Thoughput=33.60 samples/s\n",
      "INFO:gluonnlp:06:14:55 Epoch: 0, Batch: 15699/22161, Loss=1.0174, lr=0.0000162 Time cost=5.9 Thoughput=33.93 samples/s\n",
      "INFO:gluonnlp:06:15:01 Epoch: 0, Batch: 15749/22161, Loss=1.1616, lr=0.0000161 Time cost=5.8 Thoughput=34.19 samples/s\n",
      "INFO:gluonnlp:06:15:07 Epoch: 0, Batch: 15799/22161, Loss=1.1202, lr=0.0000159 Time cost=5.8 Thoughput=34.38 samples/s\n",
      "INFO:gluonnlp:06:15:13 Epoch: 0, Batch: 15849/22161, Loss=1.1154, lr=0.0000158 Time cost=5.8 Thoughput=34.63 samples/s\n",
      "INFO:gluonnlp:06:15:18 Epoch: 0, Batch: 15899/22161, Loss=1.2011, lr=0.0000157 Time cost=5.8 Thoughput=34.20 samples/s\n",
      "INFO:gluonnlp:06:15:24 Epoch: 0, Batch: 15949/22161, Loss=1.1445, lr=0.0000156 Time cost=5.8 Thoughput=34.58 samples/s\n",
      "INFO:gluonnlp:06:15:30 Epoch: 0, Batch: 15999/22161, Loss=1.2257, lr=0.0000154 Time cost=5.9 Thoughput=33.97 samples/s\n",
      "INFO:gluonnlp:06:15:36 Epoch: 0, Batch: 16049/22161, Loss=0.9484, lr=0.0000153 Time cost=5.8 Thoughput=34.20 samples/s\n",
      "INFO:gluonnlp:06:15:42 Epoch: 0, Batch: 16099/22161, Loss=1.0911, lr=0.0000152 Time cost=5.8 Thoughput=34.21 samples/s\n",
      "INFO:gluonnlp:06:15:48 Epoch: 0, Batch: 16149/22161, Loss=1.0219, lr=0.0000151 Time cost=5.8 Thoughput=34.69 samples/s\n",
      "INFO:gluonnlp:06:15:53 Epoch: 0, Batch: 16199/22161, Loss=1.0472, lr=0.0000149 Time cost=5.8 Thoughput=34.63 samples/s\n",
      "INFO:gluonnlp:06:15:59 Epoch: 0, Batch: 16249/22161, Loss=1.1054, lr=0.0000148 Time cost=5.8 Thoughput=34.40 samples/s\n",
      "INFO:gluonnlp:06:16:05 Epoch: 0, Batch: 16299/22161, Loss=1.2156, lr=0.0000147 Time cost=5.9 Thoughput=34.10 samples/s\n",
      "INFO:gluonnlp:06:16:11 Epoch: 0, Batch: 16349/22161, Loss=0.9825, lr=0.0000146 Time cost=5.9 Thoughput=34.17 samples/s\n",
      "INFO:gluonnlp:06:16:17 Epoch: 0, Batch: 16399/22161, Loss=1.0970, lr=0.0000144 Time cost=5.8 Thoughput=34.54 samples/s\n",
      "INFO:gluonnlp:06:16:23 Epoch: 0, Batch: 16449/22161, Loss=1.2098, lr=0.0000143 Time cost=5.8 Thoughput=34.21 samples/s\n",
      "INFO:gluonnlp:06:16:28 Epoch: 0, Batch: 16499/22161, Loss=1.0207, lr=0.0000142 Time cost=5.8 Thoughput=34.35 samples/s\n",
      "INFO:gluonnlp:06:16:34 Epoch: 0, Batch: 16549/22161, Loss=1.0700, lr=0.0000141 Time cost=5.8 Thoughput=34.26 samples/s\n",
      "INFO:gluonnlp:06:16:40 Epoch: 0, Batch: 16599/22161, Loss=1.3040, lr=0.0000139 Time cost=5.8 Thoughput=34.31 samples/s\n",
      "INFO:gluonnlp:06:16:46 Epoch: 0, Batch: 16649/22161, Loss=0.9734, lr=0.0000138 Time cost=5.9 Thoughput=34.14 samples/s\n",
      "INFO:gluonnlp:06:16:52 Epoch: 0, Batch: 16699/22161, Loss=1.2618, lr=0.0000137 Time cost=5.8 Thoughput=34.33 samples/s\n",
      "INFO:gluonnlp:06:16:58 Epoch: 0, Batch: 16749/22161, Loss=1.1668, lr=0.0000136 Time cost=5.8 Thoughput=34.40 samples/s\n",
      "INFO:gluonnlp:06:17:03 Epoch: 0, Batch: 16799/22161, Loss=1.2870, lr=0.0000134 Time cost=5.8 Thoughput=34.66 samples/s\n",
      "INFO:gluonnlp:06:17:09 Epoch: 0, Batch: 16849/22161, Loss=0.9725, lr=0.0000133 Time cost=5.8 Thoughput=34.56 samples/s\n",
      "INFO:gluonnlp:06:17:15 Epoch: 0, Batch: 16899/22161, Loss=1.1091, lr=0.0000132 Time cost=5.8 Thoughput=34.52 samples/s\n",
      "INFO:gluonnlp:06:17:21 Epoch: 0, Batch: 16949/22161, Loss=1.0008, lr=0.0000131 Time cost=5.8 Thoughput=34.31 samples/s\n",
      "INFO:gluonnlp:06:17:27 Epoch: 0, Batch: 16999/22161, Loss=1.0924, lr=0.0000129 Time cost=5.9 Thoughput=33.91 samples/s\n",
      "INFO:gluonnlp:06:17:32 Epoch: 0, Batch: 17049/22161, Loss=1.1818, lr=0.0000128 Time cost=5.9 Thoughput=34.06 samples/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gluonnlp:06:17:38 Epoch: 0, Batch: 17099/22161, Loss=1.0581, lr=0.0000127 Time cost=5.9 Thoughput=34.02 samples/s\n",
      "INFO:gluonnlp:06:17:44 Epoch: 0, Batch: 17149/22161, Loss=0.9211, lr=0.0000126 Time cost=5.9 Thoughput=34.05 samples/s\n",
      "INFO:gluonnlp:06:17:50 Epoch: 0, Batch: 17199/22161, Loss=1.1125, lr=0.0000124 Time cost=6.0 Thoughput=33.50 samples/s\n",
      "INFO:gluonnlp:06:17:56 Epoch: 0, Batch: 17249/22161, Loss=1.0050, lr=0.0000123 Time cost=5.9 Thoughput=33.63 samples/s\n",
      "INFO:gluonnlp:06:18:02 Epoch: 0, Batch: 17299/22161, Loss=1.0041, lr=0.0000122 Time cost=5.8 Thoughput=34.26 samples/s\n",
      "INFO:gluonnlp:06:18:08 Epoch: 0, Batch: 17349/22161, Loss=1.0046, lr=0.0000121 Time cost=5.8 Thoughput=34.43 samples/s\n",
      "INFO:gluonnlp:06:18:14 Epoch: 0, Batch: 17399/22161, Loss=0.9258, lr=0.0000119 Time cost=5.8 Thoughput=34.32 samples/s\n",
      "INFO:gluonnlp:06:18:19 Epoch: 0, Batch: 17449/22161, Loss=1.1690, lr=0.0000118 Time cost=5.8 Thoughput=34.35 samples/s\n",
      "INFO:gluonnlp:06:18:25 Epoch: 0, Batch: 17499/22161, Loss=1.0911, lr=0.0000117 Time cost=5.9 Thoughput=34.01 samples/s\n",
      "INFO:gluonnlp:06:18:31 Epoch: 0, Batch: 17549/22161, Loss=1.1869, lr=0.0000116 Time cost=5.8 Thoughput=34.59 samples/s\n",
      "INFO:gluonnlp:06:18:37 Epoch: 0, Batch: 17599/22161, Loss=1.1823, lr=0.0000114 Time cost=5.9 Thoughput=34.09 samples/s\n",
      "INFO:gluonnlp:06:18:43 Epoch: 0, Batch: 17649/22161, Loss=1.0625, lr=0.0000113 Time cost=5.8 Thoughput=34.57 samples/s\n",
      "INFO:gluonnlp:06:18:49 Epoch: 0, Batch: 17699/22161, Loss=1.2066, lr=0.0000112 Time cost=5.9 Thoughput=33.68 samples/s\n",
      "INFO:gluonnlp:06:18:55 Epoch: 0, Batch: 17749/22161, Loss=0.9980, lr=0.0000111 Time cost=5.9 Thoughput=34.12 samples/s\n",
      "INFO:gluonnlp:06:19:00 Epoch: 0, Batch: 17799/22161, Loss=1.0285, lr=0.0000109 Time cost=5.9 Thoughput=34.07 samples/s\n",
      "INFO:gluonnlp:06:19:06 Epoch: 0, Batch: 17849/22161, Loss=0.9845, lr=0.0000108 Time cost=5.8 Thoughput=34.21 samples/s\n",
      "INFO:gluonnlp:06:19:12 Epoch: 0, Batch: 17899/22161, Loss=1.0948, lr=0.0000107 Time cost=5.8 Thoughput=34.60 samples/s\n",
      "INFO:gluonnlp:06:19:18 Epoch: 0, Batch: 17949/22161, Loss=1.1205, lr=0.0000106 Time cost=5.8 Thoughput=34.35 samples/s\n",
      "INFO:gluonnlp:06:19:24 Epoch: 0, Batch: 17999/22161, Loss=0.9007, lr=0.0000104 Time cost=5.8 Thoughput=34.34 samples/s\n",
      "INFO:gluonnlp:06:19:29 Epoch: 0, Batch: 18049/22161, Loss=0.9537, lr=0.0000103 Time cost=5.8 Thoughput=34.48 samples/s\n",
      "INFO:gluonnlp:06:19:35 Epoch: 0, Batch: 18099/22161, Loss=1.0991, lr=0.0000102 Time cost=5.8 Thoughput=34.27 samples/s\n",
      "INFO:gluonnlp:06:19:41 Epoch: 0, Batch: 18149/22161, Loss=1.1164, lr=0.0000101 Time cost=5.8 Thoughput=34.26 samples/s\n",
      "INFO:gluonnlp:06:19:47 Epoch: 0, Batch: 18199/22161, Loss=1.0096, lr=0.0000099 Time cost=5.8 Thoughput=34.19 samples/s\n",
      "INFO:gluonnlp:06:19:53 Epoch: 0, Batch: 18249/22161, Loss=1.1233, lr=0.0000098 Time cost=5.9 Thoughput=34.11 samples/s\n",
      "INFO:gluonnlp:06:19:59 Epoch: 0, Batch: 18299/22161, Loss=0.9183, lr=0.0000097 Time cost=5.9 Thoughput=34.18 samples/s\n",
      "INFO:gluonnlp:06:20:05 Epoch: 0, Batch: 18349/22161, Loss=1.0238, lr=0.0000096 Time cost=5.8 Thoughput=34.26 samples/s\n",
      "INFO:gluonnlp:06:20:10 Epoch: 0, Batch: 18399/22161, Loss=1.0115, lr=0.0000094 Time cost=5.9 Thoughput=34.07 samples/s\n",
      "INFO:gluonnlp:06:20:16 Epoch: 0, Batch: 18449/22161, Loss=1.0812, lr=0.0000093 Time cost=5.9 Thoughput=33.79 samples/s\n",
      "INFO:gluonnlp:06:20:22 Epoch: 0, Batch: 18499/22161, Loss=0.9370, lr=0.0000092 Time cost=5.9 Thoughput=34.04 samples/s\n",
      "INFO:gluonnlp:06:20:28 Epoch: 0, Batch: 18549/22161, Loss=1.1761, lr=0.0000091 Time cost=5.8 Thoughput=34.40 samples/s\n",
      "INFO:gluonnlp:06:20:34 Epoch: 0, Batch: 18599/22161, Loss=1.1137, lr=0.0000089 Time cost=5.8 Thoughput=34.36 samples/s\n",
      "INFO:gluonnlp:06:20:40 Epoch: 0, Batch: 18649/22161, Loss=0.8670, lr=0.0000088 Time cost=5.8 Thoughput=34.34 samples/s\n",
      "INFO:gluonnlp:06:20:46 Epoch: 0, Batch: 18699/22161, Loss=1.1645, lr=0.0000087 Time cost=5.9 Thoughput=34.10 samples/s\n",
      "INFO:gluonnlp:06:20:51 Epoch: 0, Batch: 18749/22161, Loss=0.9721, lr=0.0000085 Time cost=5.8 Thoughput=34.44 samples/s\n",
      "INFO:gluonnlp:06:20:57 Epoch: 0, Batch: 18799/22161, Loss=1.0409, lr=0.0000084 Time cost=5.8 Thoughput=34.23 samples/s\n",
      "INFO:gluonnlp:06:21:03 Epoch: 0, Batch: 18849/22161, Loss=1.0207, lr=0.0000083 Time cost=5.8 Thoughput=34.25 samples/s\n",
      "INFO:gluonnlp:06:21:09 Epoch: 0, Batch: 18899/22161, Loss=1.1545, lr=0.0000082 Time cost=5.8 Thoughput=34.46 samples/s\n",
      "INFO:gluonnlp:06:21:15 Epoch: 0, Batch: 18949/22161, Loss=0.9204, lr=0.0000080 Time cost=5.9 Thoughput=34.02 samples/s\n",
      "INFO:gluonnlp:06:21:21 Epoch: 0, Batch: 18999/22161, Loss=1.1060, lr=0.0000079 Time cost=5.9 Thoughput=33.85 samples/s\n",
      "INFO:gluonnlp:06:21:27 Epoch: 0, Batch: 19049/22161, Loss=1.1079, lr=0.0000078 Time cost=5.9 Thoughput=33.96 samples/s\n",
      "INFO:gluonnlp:06:21:32 Epoch: 0, Batch: 19099/22161, Loss=1.0643, lr=0.0000077 Time cost=5.8 Thoughput=34.42 samples/s\n",
      "INFO:gluonnlp:06:21:38 Epoch: 0, Batch: 19149/22161, Loss=0.9997, lr=0.0000075 Time cost=5.8 Thoughput=34.49 samples/s\n",
      "INFO:gluonnlp:06:21:44 Epoch: 0, Batch: 19199/22161, Loss=1.1087, lr=0.0000074 Time cost=5.9 Thoughput=34.14 samples/s\n",
      "INFO:gluonnlp:06:21:50 Epoch: 0, Batch: 19249/22161, Loss=0.9919, lr=0.0000073 Time cost=5.8 Thoughput=34.24 samples/s\n",
      "INFO:gluonnlp:06:21:56 Epoch: 0, Batch: 19299/22161, Loss=0.9780, lr=0.0000072 Time cost=5.9 Thoughput=34.11 samples/s\n",
      "INFO:gluonnlp:06:22:02 Epoch: 0, Batch: 19349/22161, Loss=1.1976, lr=0.0000070 Time cost=5.9 Thoughput=34.04 samples/s\n",
      "INFO:gluonnlp:06:22:07 Epoch: 0, Batch: 19399/22161, Loss=1.1912, lr=0.0000069 Time cost=5.9 Thoughput=33.89 samples/s\n",
      "INFO:gluonnlp:06:22:13 Epoch: 0, Batch: 19449/22161, Loss=0.9516, lr=0.0000068 Time cost=5.9 Thoughput=33.74 samples/s\n",
      "INFO:gluonnlp:06:22:19 Epoch: 0, Batch: 19499/22161, Loss=0.9875, lr=0.0000067 Time cost=5.8 Thoughput=34.55 samples/s\n",
      "INFO:gluonnlp:06:22:25 Epoch: 0, Batch: 19549/22161, Loss=1.2626, lr=0.0000065 Time cost=5.8 Thoughput=34.21 samples/s\n",
      "INFO:gluonnlp:06:22:31 Epoch: 0, Batch: 19599/22161, Loss=0.9743, lr=0.0000064 Time cost=5.8 Thoughput=34.20 samples/s\n",
      "INFO:gluonnlp:06:22:37 Epoch: 0, Batch: 19649/22161, Loss=1.0383, lr=0.0000063 Time cost=5.9 Thoughput=34.01 samples/s\n",
      "INFO:gluonnlp:06:22:43 Epoch: 0, Batch: 19699/22161, Loss=1.3160, lr=0.0000062 Time cost=5.9 Thoughput=34.05 samples/s\n",
      "INFO:gluonnlp:06:22:49 Epoch: 0, Batch: 19749/22161, Loss=1.0556, lr=0.0000060 Time cost=5.9 Thoughput=33.76 samples/s\n",
      "INFO:gluonnlp:06:22:54 Epoch: 0, Batch: 19799/22161, Loss=0.9470, lr=0.0000059 Time cost=5.8 Thoughput=34.19 samples/s\n",
      "INFO:gluonnlp:06:23:00 Epoch: 0, Batch: 19849/22161, Loss=1.2617, lr=0.0000058 Time cost=5.8 Thoughput=34.41 samples/s\n",
      "INFO:gluonnlp:06:23:06 Epoch: 0, Batch: 19899/22161, Loss=1.1233, lr=0.0000057 Time cost=5.9 Thoughput=33.96 samples/s\n",
      "INFO:gluonnlp:06:23:12 Epoch: 0, Batch: 19949/22161, Loss=0.8976, lr=0.0000055 Time cost=5.8 Thoughput=34.42 samples/s\n",
      "INFO:gluonnlp:06:23:18 Epoch: 0, Batch: 19999/22161, Loss=1.1795, lr=0.0000054 Time cost=5.9 Thoughput=34.14 samples/s\n",
      "INFO:gluonnlp:06:23:24 Epoch: 0, Batch: 20049/22161, Loss=1.1474, lr=0.0000053 Time cost=5.8 Thoughput=34.50 samples/s\n",
      "INFO:gluonnlp:06:23:29 Epoch: 0, Batch: 20099/22161, Loss=0.8909, lr=0.0000052 Time cost=5.8 Thoughput=34.47 samples/s\n",
      "INFO:gluonnlp:06:23:35 Epoch: 0, Batch: 20149/22161, Loss=0.9457, lr=0.0000050 Time cost=5.8 Thoughput=34.72 samples/s\n",
      "INFO:gluonnlp:06:23:41 Epoch: 0, Batch: 20199/22161, Loss=1.2191, lr=0.0000049 Time cost=5.8 Thoughput=34.32 samples/s\n",
      "INFO:gluonnlp:06:23:47 Epoch: 0, Batch: 20249/22161, Loss=0.9379, lr=0.0000048 Time cost=5.9 Thoughput=33.62 samples/s\n",
      "INFO:gluonnlp:06:23:53 Epoch: 0, Batch: 20299/22161, Loss=1.3315, lr=0.0000047 Time cost=5.9 Thoughput=34.09 samples/s\n",
      "INFO:gluonnlp:06:23:59 Epoch: 0, Batch: 20349/22161, Loss=1.1800, lr=0.0000045 Time cost=5.8 Thoughput=34.37 samples/s\n",
      "INFO:gluonnlp:06:24:04 Epoch: 0, Batch: 20399/22161, Loss=0.9897, lr=0.0000044 Time cost=5.8 Thoughput=34.36 samples/s\n",
      "INFO:gluonnlp:06:24:10 Epoch: 0, Batch: 20449/22161, Loss=1.1106, lr=0.0000043 Time cost=5.8 Thoughput=34.70 samples/s\n",
      "INFO:gluonnlp:06:24:16 Epoch: 0, Batch: 20499/22161, Loss=1.1035, lr=0.0000042 Time cost=5.8 Thoughput=34.21 samples/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gluonnlp:06:24:22 Epoch: 0, Batch: 20549/22161, Loss=1.2698, lr=0.0000040 Time cost=5.9 Thoughput=34.00 samples/s\n",
      "INFO:gluonnlp:06:24:28 Epoch: 0, Batch: 20599/22161, Loss=0.8955, lr=0.0000039 Time cost=5.9 Thoughput=34.14 samples/s\n",
      "INFO:gluonnlp:06:24:34 Epoch: 0, Batch: 20649/22161, Loss=0.9238, lr=0.0000038 Time cost=5.8 Thoughput=34.19 samples/s\n",
      "INFO:gluonnlp:06:24:39 Epoch: 0, Batch: 20699/22161, Loss=0.9906, lr=0.0000037 Time cost=5.8 Thoughput=34.46 samples/s\n",
      "INFO:gluonnlp:06:24:45 Epoch: 0, Batch: 20749/22161, Loss=1.0895, lr=0.0000035 Time cost=5.8 Thoughput=34.78 samples/s\n",
      "INFO:gluonnlp:06:24:51 Epoch: 0, Batch: 20799/22161, Loss=1.0938, lr=0.0000034 Time cost=5.8 Thoughput=34.60 samples/s\n",
      "INFO:gluonnlp:06:24:57 Epoch: 0, Batch: 20849/22161, Loss=1.0016, lr=0.0000033 Time cost=5.9 Thoughput=34.05 samples/s\n",
      "INFO:gluonnlp:06:25:03 Epoch: 0, Batch: 20899/22161, Loss=1.0262, lr=0.0000032 Time cost=5.8 Thoughput=34.28 samples/s\n",
      "INFO:gluonnlp:06:25:09 Epoch: 0, Batch: 20949/22161, Loss=1.1606, lr=0.0000030 Time cost=6.3 Thoughput=31.90 samples/s\n",
      "INFO:gluonnlp:06:25:15 Epoch: 0, Batch: 20999/22161, Loss=1.0566, lr=0.0000029 Time cost=5.8 Thoughput=34.48 samples/s\n",
      "INFO:gluonnlp:06:25:21 Epoch: 0, Batch: 21049/22161, Loss=1.0813, lr=0.0000028 Time cost=5.8 Thoughput=34.59 samples/s\n",
      "INFO:gluonnlp:06:25:26 Epoch: 0, Batch: 21099/22161, Loss=1.0253, lr=0.0000027 Time cost=5.8 Thoughput=34.20 samples/s\n",
      "INFO:gluonnlp:06:25:32 Epoch: 0, Batch: 21149/22161, Loss=1.1315, lr=0.0000025 Time cost=5.8 Thoughput=34.49 samples/s\n",
      "INFO:gluonnlp:06:25:38 Epoch: 0, Batch: 21199/22161, Loss=0.8914, lr=0.0000024 Time cost=5.8 Thoughput=34.30 samples/s\n",
      "INFO:gluonnlp:06:25:44 Epoch: 0, Batch: 21249/22161, Loss=1.0583, lr=0.0000023 Time cost=5.8 Thoughput=34.34 samples/s\n",
      "INFO:gluonnlp:06:25:50 Epoch: 0, Batch: 21299/22161, Loss=0.8743, lr=0.0000022 Time cost=5.9 Thoughput=33.98 samples/s\n",
      "INFO:gluonnlp:06:25:56 Epoch: 0, Batch: 21349/22161, Loss=0.9544, lr=0.0000020 Time cost=5.8 Thoughput=34.32 samples/s\n",
      "INFO:gluonnlp:06:26:01 Epoch: 0, Batch: 21399/22161, Loss=0.8641, lr=0.0000019 Time cost=5.9 Thoughput=33.82 samples/s\n",
      "INFO:gluonnlp:06:26:07 Epoch: 0, Batch: 21449/22161, Loss=0.9191, lr=0.0000018 Time cost=5.9 Thoughput=34.04 samples/s\n",
      "INFO:gluonnlp:06:26:13 Epoch: 0, Batch: 21499/22161, Loss=1.0064, lr=0.0000017 Time cost=5.9 Thoughput=33.85 samples/s\n",
      "INFO:gluonnlp:06:26:19 Epoch: 0, Batch: 21549/22161, Loss=0.9501, lr=0.0000015 Time cost=5.9 Thoughput=33.85 samples/s\n",
      "INFO:gluonnlp:06:26:25 Epoch: 0, Batch: 21599/22161, Loss=0.8750, lr=0.0000014 Time cost=5.8 Thoughput=34.30 samples/s\n",
      "INFO:gluonnlp:06:26:31 Epoch: 0, Batch: 21649/22161, Loss=1.1342, lr=0.0000013 Time cost=5.9 Thoughput=34.18 samples/s\n",
      "INFO:gluonnlp:06:26:37 Epoch: 0, Batch: 21699/22161, Loss=1.0121, lr=0.0000012 Time cost=5.9 Thoughput=34.11 samples/s\n",
      "INFO:gluonnlp:06:26:43 Epoch: 0, Batch: 21749/22161, Loss=0.9032, lr=0.0000010 Time cost=5.9 Thoughput=33.98 samples/s\n",
      "INFO:gluonnlp:06:26:49 Epoch: 0, Batch: 21799/22161, Loss=0.9529, lr=0.0000009 Time cost=5.9 Thoughput=33.75 samples/s\n",
      "INFO:gluonnlp:06:26:54 Epoch: 0, Batch: 21849/22161, Loss=0.9315, lr=0.0000008 Time cost=5.9 Thoughput=33.89 samples/s\n",
      "INFO:gluonnlp:06:27:00 Epoch: 0, Batch: 21899/22161, Loss=1.1602, lr=0.0000007 Time cost=5.8 Thoughput=34.49 samples/s\n",
      "INFO:gluonnlp:06:27:06 Epoch: 0, Batch: 21949/22161, Loss=1.0377, lr=0.0000005 Time cost=5.9 Thoughput=34.17 samples/s\n",
      "INFO:gluonnlp:06:27:12 Epoch: 0, Batch: 21999/22161, Loss=1.0001, lr=0.0000004 Time cost=5.8 Thoughput=34.31 samples/s\n",
      "INFO:gluonnlp:06:27:18 Epoch: 0, Batch: 22049/22161, Loss=1.0985, lr=0.0000003 Time cost=5.9 Thoughput=33.74 samples/s\n",
      "INFO:gluonnlp:06:27:24 Epoch: 0, Batch: 22099/22161, Loss=0.9491, lr=0.0000002 Time cost=5.9 Thoughput=34.10 samples/s\n",
      "INFO:gluonnlp:06:27:30 Epoch: 0, Batch: 22149/22161, Loss=0.8119, lr=0.0000000 Time cost=5.8 Thoughput=34.25 samples/s\n",
      "INFO:gluonnlp:06:27:31 Time cost=2588.80 s, Thoughput=34.24 samples/s\n",
      "INFO:gluonnlp:06:27:32 Loading dev data...\n",
      "INFO:gluonnlp:06:27:32 Number of records in dev data:10570\n",
      "Done! Transform dataset costs 6.76 seconds.\n",
      "INFO:gluonnlp:06:28:03 The number of examples after preprocessing:10833\n",
      "INFO:gluonnlp:06:28:03 start prediction\n",
      "INFO:gluonnlp:06:28:55 Time cost=51.48 s, Thoughput=210.42 samples/s\n",
      "INFO:gluonnlp:06:28:55 Get prediction results...\n",
      "INFO:gluonnlp:06:29:34 {'exact_match': 80.4162724692526, 'f1': 87.62947592906423}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "!python finetune_squad.py --epochs 1 --batch_size 4 --bert_model 'bert_12_768_12' --gpu 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2770.2639062404633\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Resources\n",
    "\n",
    "- Dive into Deep Learning http://d2l.ai/\n",
    "\n",
    "- GluonNLP http://gluon-nlp.mxnet.io/\n",
    "- GluonCV http://gluon-cv.mxnet.io/\n",
    "- GluonTS https://gluon-ts.mxnet.io/\n",
    "- Deep Graph Libray https://www.dgl.ai/\n",
    "- MXNet Forum https://discuss.mxnet.io/\n",
    "\n",
    "- Amazon SageMaker https://aws.amazon.com/sagemaker/\n",
    "- Amazon SageMaker Python SDK https://sagemaker.readthedocs.io/\n",
    "- Amazon SageMaker Developer Guide https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-dg.pdf\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "[1] Devlin, Jacob, et al. \"Bert:\n",
    "Pre-training of deep\n",
    "bidirectional transformers for language understanding.\"\n",
    "arXiv preprint\n",
    "arXiv:1810.04805 (2018).\n",
    "\n",
    "[2] Peters,\n",
    "Matthew E., et al. \"Deep contextualized word representations.\" arXiv\n",
    "preprint\n",
    "arXiv:1802.05365 (2018).\n",
    "\n",
    "[3] Hendrycks, Dan, and Kevin Gimpel. \"Gaussian error linear units (gelus).\" arXiv preprint arXiv:1606.08415 (2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
