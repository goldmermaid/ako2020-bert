{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hands-on: Deploying Question Answering with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-trained language representations have been shown to improve many downstream NLP tasks such as question answering, and natural language inference. Devlin, Jacob, et al proposed BERT [1] (Bidirectional Encoder Representations from Transformers), which fine-tunes deep bidirectional representations on a wide range of tasks with minimal task-specific parameters, and obtained state- of-the-art results.\n",
    "\n",
    "After finishing training QA with BERT (the previous notebook \"QA_Training.ipydb\"), let us load a trained model to perform inference on the SQuAD dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick overview: an example from SQuAD dataset is like below:\n",
    "\n",
    "    (2, \n",
    "    '56be4db0acb8001400a502ee', \n",
    "    'Where did Super Bowl 50 take place?', \n",
    "\n",
    "    'Super Bowl 50 was an American football game to determine the champion of the National \n",
    "    Football League (NFL) for the 2015 season. The American Football Conference (AFC) \n",
    "    champion Denver Broncos defeated the National Football Conference (NFC) champion \n",
    "    Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played \n",
    "    on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, \n",
    "    California. As this was the 50th Super Bowl, the league emphasized the \"golden \n",
    "    anniversary\" with various gold-themed initiatives, as well as temporarily suspending \n",
    "    the tradition of naming each Super Bowl game with Roman numerals (under which the \n",
    "    game would have been known as \"Super Bowl L\"), so that the logo could prominently \n",
    "    feature the Arabic numerals 50.', \n",
    "\n",
    "    ['Santa Clara, California', \"Levi's Stadium\", \"Levi's Stadium \n",
    "    in the San Francisco Bay Area at Santa Clara, California.\"], \n",
    "\n",
    "    [403, 355, 355])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy on SageMaker\n",
    "\n",
    "1. Preparing functions for inference \n",
    "2. Saving the model parameters\n",
    "3. Building a docker container with dependencies installed\n",
    "4. Launching a serving end-point with SageMaker SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preparing functions for inference\n",
    "\n",
    "Two functions: \n",
    "1. ```model_fn``` to load model parameters\n",
    "2. ```transform_fn(``` to run model inference given an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/serve.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/serve.py\n",
    "\n",
    "import collections, json, logging, warnings\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "\n",
    "import gluonnlp as nlp\n",
    "import mxnet as mx\n",
    "from mxnet.gluon import Block, nn\n",
    "# import bert \n",
    "from qa import preprocess_dataset, SQuADTransform\n",
    "import bert_qa_evaluate\n",
    "\n",
    "\n",
    "\n",
    "class BertForQA(Block):\n",
    "    \"\"\"Model for SQuAD task with BERT.\n",
    "    The model feeds token ids and token type ids into BERT to get the\n",
    "    pooled BERT sequence representation, then apply a Dense layer for QA task.\n",
    "    Parameters\n",
    "    ----------\n",
    "    bert: BERTModel\n",
    "        Bidirectional encoder with transformer.\n",
    "    prefix : str or None\n",
    "        See document of `mx.gluon.Block`.\n",
    "    params : ParameterDict or None\n",
    "        See document of `mx.gluon.Block`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bert, prefix=None, params=None):\n",
    "        super(BertForQA, self).__init__(prefix=prefix, params=params)\n",
    "        self.bert = bert\n",
    "        with self.name_scope():\n",
    "            self.span_classifier = nn.Dense(units=2, flatten=False)\n",
    "\n",
    "    def forward(self, inputs, token_types, valid_length=None):  # pylint: disable=arguments-differ\n",
    "        \"\"\"Generate the unnormalized score for the given the input sequences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : NDArray, shape (batch_size, seq_length)\n",
    "            Input words for the sequences.\n",
    "        token_types : NDArray, shape (batch_size, seq_length)\n",
    "            Token types for the sequences, used to indicate whether the word belongs to the\n",
    "            first sentence or the second one.\n",
    "        valid_length : NDArray or None, shape (batch_size,)\n",
    "            Valid length of the sequence. This is used to mask the padded tokens.\n",
    "        Returns\n",
    "        -------\n",
    "        outputs : NDArray\n",
    "            Shape (batch_size, seq_length, 2)\n",
    "        \"\"\"\n",
    "        bert_output = self.bert(inputs, token_types, valid_length)\n",
    "        output = self.span_classifier(bert_output)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "def get_all_results(net, vocab, squadTransform, test_dataset, ctx = mx.cpu()):\n",
    "    all_results = collections.defaultdict(list)\n",
    "    \n",
    "    def _vocab_lookup(example_id, subwords, type_ids, length, start, end):\n",
    "        indices = vocab[subwords]\n",
    "        return example_id, indices, type_ids, length, start, end\n",
    "    \n",
    "    dev_data_transform, _ = preprocess_dataset(test_dataset, squadTransform)\n",
    "    dev_data_transform = dev_data_transform.transform(_vocab_lookup, lazy=False)\n",
    "    dev_dataloader = mx.gluon.data.DataLoader(dev_data_transform, batch_size=1, shuffle=False)\n",
    "    \n",
    "    for data in dev_dataloader:\n",
    "        example_ids, inputs, token_types, valid_length, _, _ = data\n",
    "        batch_size = inputs.shape[0]\n",
    "        output = net(inputs.astype('float32').as_in_context(ctx),\n",
    "                     token_types.astype('float32').as_in_context(ctx),\n",
    "                     valid_length.astype('float32').as_in_context(ctx))\n",
    "        pred_start, pred_end = mx.nd.split(output, axis=2, num_outputs=2)\n",
    "        example_ids = example_ids.asnumpy().tolist()\n",
    "        pred_start = pred_start.reshape(batch_size, -1).asnumpy()\n",
    "        pred_end = pred_end.reshape(batch_size, -1).asnumpy()\n",
    "\n",
    "        for example_id, start, end in zip(example_ids, pred_start, pred_end):\n",
    "            all_results[example_id].append(bert_qa_evaluate.PredResult(start=start, end=end))\n",
    "    return(all_results)\n",
    "\n",
    "\n",
    "def _test_example_transform(test_examples):\n",
    "    \"\"\"\n",
    "    Change test examples to a format like SQUAD data.\n",
    "    Parameters\n",
    "    ---------- \n",
    "    test_examples: a list of (question, context) tuple. \n",
    "        Example: [('Which NFL team represented the AFC at Super Bowl 50?',\n",
    "                 'Super Bowl 50 was an American football game ......),\n",
    "                  ('Where did Super Bowl 50 take place?',,\n",
    "                 'Super Bowl 50 was ......),\n",
    "                 ......]\n",
    "    Returns\n",
    "    ----------\n",
    "    test_examples_tuples : a list of SQUAD tuples\n",
    "    \"\"\"\n",
    "    test_examples_tuples = []\n",
    "    i = 0\n",
    "    for test in test_examples:\n",
    "        question, context = test[0], test[1]  # test.split(\" [CONTEXT] \")\n",
    "        tup = (i, \"\", question, context, [], [])\n",
    "        test_examples_tuples.append(tup)\n",
    "        i += 1\n",
    "    return(test_examples_tuples)\n",
    "\n",
    "\n",
    "def model_fn(model_dir = \"\", params_path = \"bert_qa-7eb11865.params\"):\n",
    "    \"\"\"\n",
    "    Load the gluon model. Called once when hosting service starts.\n",
    "    :param: model_dir The directory where model files are stored.\n",
    "    :return: a Gluon model, and the vocabulary\n",
    "    \"\"\"\n",
    "    bert_model, vocab = nlp.model.get_model('bert_12_768_12',\n",
    "                                        dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                        use_classifier=False,\n",
    "                                        use_decoder=False,\n",
    "                                        use_pooler=False,\n",
    "                                        pretrained=False)\n",
    "    net = BertForQA(bert_model)\n",
    "    if len(model_dir) > 0:\n",
    "        params_path = model_dir + \"/\" +params_path\n",
    "    net.load_parameters(params_path, ctx=mx.cpu())\n",
    "    \n",
    "    tokenizer = nlp.data.BERTTokenizer(vocab,  lower=True)\n",
    "    transform = SQuADTransform(tokenizer, is_pad=False, is_training=False, do_lookup=False)\n",
    "    return net, vocab, transform\n",
    "\n",
    "\n",
    "def transform_fn(model, input_data, input_content_type=None, output_content_type=None):\n",
    "    \"\"\"\n",
    "    Transform a request using the Gluon model. Called once per request.\n",
    "    :param model: The Gluon model and the vocab\n",
    "    :param dataset: The request payload\n",
    "    \n",
    "        Example:\n",
    "        ## (example_id, [question, content], ques_cont_token_types, valid_length, _, _)\n",
    "\n",
    "\n",
    "        (2, \n",
    "        '56be4db0acb8001400a502ee', \n",
    "        'Where did Super Bowl 50 take place?', \n",
    "        \n",
    "        'Super Bowl 50 was an American football game to determine the champion of the National \n",
    "        Football League (NFL) for the 2015 season. The American Football Conference (AFC) \n",
    "        champion Denver Broncos defeated the National Football Conference (NFC) champion \n",
    "        Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played \n",
    "        on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, \n",
    "        California. As this was the 50th Super Bowl, the league emphasized the \"golden \n",
    "        anniversary\" with various gold-themed initiatives, as well as temporarily suspending \n",
    "        the tradition of naming each Super Bowl game with Roman numerals (under which the \n",
    "        game would have been known as \"Super Bowl L\"), so that the logo could prominently \n",
    "        feature the Arabic numerals 50.', \n",
    "        \n",
    "        ['Santa Clara, California', \"Levi's Stadium\", \"Levi's Stadium \n",
    "        in the San Francisco Bay Area at Santa Clara, California.\"], \n",
    "        \n",
    "        [403, 355, 355])\n",
    "\n",
    "    :param input_content_type: The request content type, assume json\n",
    "    :param output_content_type: The (desired) response content type, assume json\n",
    "    :return: response payload and content type.\n",
    "    \"\"\"\n",
    "    net, vocab, squadTransform = model\n",
    "#     data = input_data\n",
    "    data = json.loads(input_data)\n",
    "#     test_examples_tuples = [(i, \"\", question, content, [], [])]\n",
    "#     question, context = data #.split(\" [CONTEXT] \")\n",
    "#     tup = (0, \"\", question, context, [], [])\n",
    "    test_examples_tuples = _test_example_transform(data)\n",
    "    test_dataset = mx.gluon.data.SimpleDataset(test_examples_tuples)  # [tup]\n",
    "    all_results = get_all_results(net, vocab, squadTransform, test_dataset, ctx=mx.cpu())\n",
    "    all_predictions = collections.defaultdict(list) # collections.OrderedDict()\n",
    "    data_transform = test_dataset.transform(squadTransform._transform)\n",
    "    for features in data_transform:\n",
    "        f_id = features[0].example_id\n",
    "        results = all_results[f_id]\n",
    "        prediction, nbest = bert_qa_evaluate.predict(\n",
    "            features=features,\n",
    "            results=results,\n",
    "            tokenizer=nlp.data.BERTBasicTokenizer(vocab))        \n",
    "        nbest_prediction = [] \n",
    "        for i in range(3):\n",
    "            nbest_prediction.append('%.2f%% \\t %s'%(nbest[i][1] * 100, nbest[i][0]))\n",
    "        all_predictions[f_id] = nbest_prediction\n",
    "    response_body = json.dumps(all_predictions)\n",
    "    return response_body, output_content_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Saving the model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to zip the BERT model parameters, vocabulary file, and all the inference files (```code/serve.py```, ```bert/data/qa.py```, ```bert_qa_evaluate.py```) to a ```model.tar.gz``` file. (Note that the ```serve.py``` is the \"entry_point\" for Sagemaker to do the inference, and it needs to be under ```code/``` directory.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "with tarfile.open(\"model.tar.gz\", \"w:gz\") as tar:\n",
    "#     tar.add(\"code/serve.py\")\n",
    "#     tar.add(\"bert/data/qa.py\")\n",
    "#     tar.add(\"bert_qa_evaluate.py\")\n",
    "#     tar.add(\"bert_qa-7eb11865.params\")\n",
    "#     tar.add(\"vocab.json\")\n",
    "    tar.add(\"net.params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Building a docker container with dependencies installed\n",
    "\n",
    "Let's prepare a docker container with all the dependencies required for model inference. Here we build a docker container based on the SageMaker MXNet inference container, and you can find the list of all available inference containers at https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html\n",
    "\n",
    "Here we use local mode for demonstration purpose. To deploy on actual instances, you need to login into AWS elastic container registry (ECR) service, and push the container to ECR. \n",
    "\n",
    "```\n",
    "docker build -t $YOUR_EDR_DOCKER_TAG . -f Dockerfile\n",
    "$(aws ecr get-login --no-include-email --region $YOUR_REGION)\n",
    "docker push $YOUR_EDR_DOCKER_TAG\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "FROM nvidia/cuda:10.1-cudnn7-runtime-ubuntu16.04\n",
    "\n",
    "LABEL maintainer=\"Amazon AI\"\n",
    "\n",
    "# Specify accept-bind-to-port LABEL for inference pipelines to use SAGEMAKER_BIND_TO_PORT\n",
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipeline-real-time.html\n",
    "LABEL com.amazonaws.sagemaker.capabilities.accept-bind-to-port=true\n",
    "\n",
    "ARG MMS_VERSION=1.0.8\n",
    "ARG MX_URL=https://aws-mxnet-pypi.s3-us-west-2.amazonaws.com/1.6.0/aws_mxnet_cu101mkl-1.6.0-py2.py3-none-manylinux1_x86_64.whl\n",
    "ARG PYTHON=python3\n",
    "ARG PYTHON_PIP=python3-pip\n",
    "ARG PIP=pip3\n",
    "ARG PYTHON_VERSION=3.6.8\n",
    "\n",
    "ENV PYTHONDONTWRITEBYTECODE=1 \\\n",
    "    PYTHONUNBUFFERED=1 \\\n",
    "    LD_LIBRARY_PATH=\"${LD_LIBRARY_PATH}:/usr/local/lib\" \\\n",
    "    PYTHONIOENCODING=UTF-8 \\\n",
    "    LANG=C.UTF-8 \\\n",
    "    LC_ALL=C.UTF-8 \\\n",
    "    TEMP=/home/model-server/tmp \\\n",
    "    CLOUD_PATH=\"/opt/ml/code\"\n",
    "\n",
    "RUN apt-get update \\\n",
    " && apt-get -y install --no-install-recommends \\\n",
    "    build-essential \\\n",
    "    ca-certificates \\\n",
    "    curl \\\n",
    "    git \\\n",
    "    libopencv-dev \\\n",
    "    openjdk-8-jdk-headless \\\n",
    "    vim \\\n",
    "    wget \\\n",
    "    zlib1g-dev \\\n",
    " && apt-get clean \\\n",
    " && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "RUN wget https://www.python.org/ftp/python/$PYTHON_VERSION/Python-$PYTHON_VERSION.tgz \\\n",
    " && tar -xvf Python-$PYTHON_VERSION.tgz \\\n",
    " && cd Python-$PYTHON_VERSION \\\n",
    " && ./configure \\\n",
    " && make \\\n",
    " && make install \\\n",
    " && apt-get update \\\n",
    " && apt-get install -y --no-install-recommends \\\n",
    "    libreadline-gplv2-dev \\\n",
    "    libncursesw5-dev \\\n",
    "    libssl-dev \\\n",
    "    libsqlite3-dev \\\n",
    "    tk-dev \\\n",
    "    libgdbm-dev \\\n",
    "    libc6-dev \\\n",
    "    libbz2-dev \\\n",
    " && make \\\n",
    " && make install \\\n",
    " && rm -rf ../Python-$PYTHON_VERSION* \\\n",
    " && ln -s /usr/local/bin/pip3 /usr/bin/pip\n",
    "\n",
    "RUN ln -s $(which ${PYTHON}) /usr/local/bin/python\n",
    "\n",
    "RUN ${PIP} --no-cache-dir install --upgrade \\\n",
    "    pip \\\n",
    "    setuptools\n",
    "\n",
    "WORKDIR /\n",
    "\n",
    "RUN ${PIP} install --no-cache-dir \\\n",
    "    ${MX_URL} \\\n",
    "    git+git://github.com/dmlc/gluon-nlp.git@v0.9.0 \\\n",
    "#     gluoncv==0.6.0 \\\n",
    "    mxnet-model-server==$MMS_VERSION \\\n",
    "    keras-mxnet==2.2.4.1 \\\n",
    "    numpy==1.17.4 \\\n",
    "    onnx==1.4.1 \\\n",
    "    \"sagemaker-mxnet-inference<2\"\n",
    "\n",
    "\n",
    "RUN useradd -m model-server \\\n",
    " && mkdir -p /home/model-server/tmp \\\n",
    " && chown -R model-server /home/model-server\n",
    "\n",
    "COPY mms-entrypoint.py /usr/local/bin/dockerd-entrypoint.py\n",
    "COPY config.properties /home/model-server\n",
    "COPY code/serve.py $CLOUD_PATH/serve.py\n",
    "COPY bert_qa_evaluate.py $CLOUD_PATH/bert_qa_evaluate.py\n",
    "COPY qa.py $CLOUD_PATH/qa.py\n",
    "RUN chmod +x /usr/local/bin/dockerd-entrypoint.py\n",
    "RUN curl https://aws-dlc-licenses.s3.amazonaws.com/aws-mxnet-1.6.0/license.txt -o /license.txt\n",
    "\n",
    "\n",
    "EXPOSE 8080 8081\n",
    "ENTRYPOINT [\"python\", \"/usr/local/bin/dockerd-entrypoint.py\"]\n",
    "CMD [\"mxnet-model-server\", \"--start\", \"--mms-config\", \"/home/model-server/config.properties\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting build.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile build.sh\n",
    "\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "# This script shows how to build the Docker image and push it to ECR to be ready for use\n",
    "# by SageMaker.\n",
    "\n",
    "# The arguments to this script are the image name and application name\n",
    "image=$1\n",
    "app=$2\n",
    "\n",
    "chmod +x $app/train\n",
    "chmod +x $app/serve\n",
    "\n",
    "# Get the account number associated with the current IAM credentials\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration\n",
    "region=$(aws configure get region)\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${image}:latest\"\n",
    "\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${image}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${image}\" > /dev/null\n",
    "fi\n",
    "\n",
    "\n",
    "# Edit ECR policy permission rights\n",
    "aws ecr set-repository-policy --repository-name \"${image}\" --policy-text ecr_policy.json\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "docker build  -t ${image} --build-arg APP=$app .\n",
    "docker tag ${image} ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `image_name` as \"kdd2020nlp\", and application name as \"question_answering\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An error occurred (InvalidParameterException) when calling the SetRepositoryPolicy operation: Invalid parameter at 'PolicyText' failed to satisfy constraint: 'Invalid repository policy provided'\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "Sending build context to Docker daemon   1.28GB\n",
      "Step 1/27 : FROM nvidia/cuda:10.1-cudnn7-runtime-ubuntu16.04\n",
      " ---> e11e11484e2e\n",
      "Step 2/27 : LABEL maintainer=\"Amazon AI\"\n",
      " ---> Using cache\n",
      " ---> 565649be145a\n",
      "Step 3/27 : LABEL com.amazonaws.sagemaker.capabilities.accept-bind-to-port=true\n",
      " ---> Using cache\n",
      " ---> bba5a1546099\n",
      "Step 4/27 : ARG MMS_VERSION=1.0.8\n",
      " ---> Using cache\n",
      " ---> 2034f1d30c2e\n",
      "Step 5/27 : ARG MX_URL=https://aws-mxnet-pypi.s3-us-west-2.amazonaws.com/1.6.0/aws_mxnet_cu101mkl-1.6.0-py2.py3-none-manylinux1_x86_64.whl\n",
      " ---> Using cache\n",
      " ---> 08afaf84a69c\n",
      "Step 6/27 : ARG PYTHON=python3\n",
      " ---> Using cache\n",
      " ---> f0f7453df285\n",
      "Step 7/27 : ARG PYTHON_PIP=python3-pip\n",
      " ---> Using cache\n",
      " ---> 4af5eabed31e\n",
      "Step 8/27 : ARG PIP=pip3\n",
      " ---> Using cache\n",
      " ---> 1537b7495b52\n",
      "Step 9/27 : ARG PYTHON_VERSION=3.6.8\n",
      " ---> Using cache\n",
      " ---> 8b11bcdd442c\n",
      "Step 10/27 : ENV PYTHONDONTWRITEBYTECODE=1     PYTHONUNBUFFERED=1     LD_LIBRARY_PATH=\"${LD_LIBRARY_PATH}:/usr/local/lib\"     PYTHONIOENCODING=UTF-8     LANG=C.UTF-8     LC_ALL=C.UTF-8     TEMP=/home/model-server/tmp     CLOUD_PATH=\"/opt/ml/code\"\n",
      " ---> Using cache\n",
      " ---> 33b949833177\n",
      "Step 11/27 : RUN apt-get update  && apt-get -y install --no-install-recommends     build-essential     ca-certificates     curl     git     libopencv-dev     openjdk-8-jdk-headless     vim     wget     zlib1g-dev  && apt-get clean  && rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> 364d433c89bc\n",
      "Step 12/27 : RUN wget https://www.python.org/ftp/python/$PYTHON_VERSION/Python-$PYTHON_VERSION.tgz  && tar -xvf Python-$PYTHON_VERSION.tgz  && cd Python-$PYTHON_VERSION  && ./configure  && make  && make install  && apt-get update  && apt-get install -y --no-install-recommends     libreadline-gplv2-dev     libncursesw5-dev     libssl-dev     libsqlite3-dev     tk-dev     libgdbm-dev     libc6-dev     libbz2-dev  && make  && make install  && rm -rf ../Python-$PYTHON_VERSION*  && ln -s /usr/local/bin/pip3 /usr/bin/pip\n",
      " ---> Using cache\n",
      " ---> 0742daf19956\n",
      "Step 13/27 : RUN ln -s $(which ${PYTHON}) /usr/local/bin/python\n",
      " ---> Using cache\n",
      " ---> 1ec1f23e606e\n",
      "Step 14/27 : RUN ${PIP} --no-cache-dir install --upgrade     pip     setuptools\n",
      " ---> Using cache\n",
      " ---> 4703c6c39616\n",
      "Step 15/27 : WORKDIR /\n",
      " ---> Using cache\n",
      " ---> bcbda2962106\n",
      "Step 16/27 : RUN ${PIP} install --no-cache-dir     ${MX_URL}     git+git://github.com/dmlc/gluon-nlp.git@v0.9.0     mxnet-model-server==$MMS_VERSION     keras-mxnet==2.2.4.1     numpy==1.17.4     onnx==1.4.1     \"sagemaker-mxnet-inference<2\"\n",
      " ---> Using cache\n",
      " ---> d2a9deb694fe\n",
      "Step 17/27 : RUN useradd -m model-server  && mkdir -p /home/model-server/tmp  && chown -R model-server /home/model-server\n",
      " ---> Using cache\n",
      " ---> 5d4a30e5fda7\n",
      "Step 18/27 : COPY mms-entrypoint.py /usr/local/bin/dockerd-entrypoint.py\n",
      " ---> b5144cd82a78\n",
      "Step 19/27 : COPY config.properties /home/model-server\n",
      " ---> 10881d382025\n",
      "Step 20/27 : COPY code/serve.py $CLOUD_PATH/serve.py\n",
      " ---> 091c957b180e\n",
      "Step 21/27 : COPY bert_qa_evaluate.py $CLOUD_PATH/bert_qa_evaluate.py\n",
      " ---> 214db4e82eaf\n",
      "Step 22/27 : COPY qa.py $CLOUD_PATH/qa.py\n",
      " ---> 55f198a6c881\n",
      "Step 23/27 : RUN chmod +x /usr/local/bin/dockerd-entrypoint.py\n",
      " ---> Running in 5c6957435ff7\n",
      "Removing intermediate container 5c6957435ff7\n",
      " ---> beea4e51e943\n",
      "Step 24/27 : RUN curl https://aws-dlc-licenses.s3.amazonaws.com/aws-mxnet-1.6.0/license.txt -o /license.txt\n",
      " ---> Running in 5f8e036b91ef\n",
      "\u001b[91m  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "          \u001b[0m\u001b[91m                       Dload  Uploa\u001b[0m\u001b[91md   Total \u001b[0m\u001b[91m  Spent\u001b[0m\u001b[91m    Left  S\u001b[0m\u001b[91mpeed\n",
      "100 11956  100 11956    0     0   114k      0 --:--:-- --:--:-- --:--:--  114k \u001b[0m\u001b[91m 0 --:--\u001b[0m\u001b[91m:-- --:\u001b[0m\u001b[91m--:-- --:-\u001b[0m\u001b[91m-:--     0\u001b[0m\u001b[91m\n",
      "\u001b[0mRemoving intermediate container 5f8e036b91ef\n",
      " ---> f1ae8dc98f2a\n",
      "Step 25/27 : EXPOSE 8080 8081\n",
      " ---> Running in 9394631b77cd\n",
      "Removing intermediate container 9394631b77cd\n",
      " ---> 1028b9511f36\n",
      "Step 26/27 : ENTRYPOINT [\"python\", \"/usr/local/bin/dockerd-entrypoint.py\"]\n",
      " ---> Running in 75223f46bc7b\n",
      "Removing intermediate container 75223f46bc7b\n",
      " ---> c9fd1e87b9d0\n",
      "Step 27/27 : CMD [\"mxnet-model-server\", \"--start\", \"--mms-config\", \"/home/model-server/config.properties\"]\n",
      " ---> Running in e5ab998640d1\n",
      "Removing intermediate container e5ab998640d1\n",
      " ---> 26472107ee5d\n",
      "[Warning] One or more build-args [APP] were not consumed\n",
      "Successfully built 26472107ee5d\n",
      "Successfully tagged kdd2020nlp:latest\n"
     ]
    }
   ],
   "source": [
    "!bash build.sh kdd2020nlp question_answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Launching a serving end-point with SageMaker SDK\n",
    "\n",
    "We create a MXNet model which can be deployed later, by specifying the docker image, and entry point for the inference code. If ```serve.py``` does not work, use ```dummy_hosting_module.py``` for debugging purpose. \n",
    "\n",
    "#### Creating the Session\n",
    "\n",
    "The session remembers our connection parameters to Amazon SageMaker. We'll use it to perform all of our SageMaker operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "\n",
    "sess = sage.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the account, region and ECR address\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sess.boto_session.region_name\n",
    "image_name = \"kdd2020nlp\"\n",
    "ecr_image = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account, region, image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uploading model\n",
    "\n",
    "We can upload the trained model to the corresponding S3 bucket: https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-383827541835/sagemaker-deploy-gluoncv/data/?region=us-east-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-us-east-1-383827541835'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.default_bucket()\n",
    "sess.update_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket_name = \"kdd2020\"\n",
    "model_path = \"s3://{}/{}/model\".format(sess.default_bucket(), s3_bucket_name)\n",
    "os.path.join(model_path, \"model.tar.gz\")\n",
    "model_prefix = s3_bucket + \"/model\"\n",
    "train_data_local = \"./data/minc-2500/train\"\n",
    "train_data_dir_prefix = s3_bucket + \"/data/train\"\n",
    "\n",
    "\n",
    "# model_local_path = \"model_output\"\n",
    "train_data_upload = sess.upload_data(path=train_data_local, \n",
    "#                                 bucket=s3_bucket, \n",
    "                                key_prefix=train_data_dir_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.mxnet.model import MXNetModel\n",
    "sagemaker_model = MXNetModel(model_data='file:///home/ec2-user/SageMaker/ako2020-bert/tutorial/model.tar.gz',\n",
    "                             image=ecr_image,\n",
    "                             role=sagemaker.get_execution_role(), \n",
    "                             py_version='py3',            # python version\n",
    "                             entry_point='serve.py',\n",
    "                             source_dir='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 'local' mode to test our deployment code, where the inference happens on the current instance.\n",
    "If you are ready to deploy the model on a new instance, change the `instance_type` argument to values such as `ml.c4.xlarge`.\n",
    "\n",
    "Here we use 'local' mode for testing, for real instances use c5.2xlarge, p2.xlarge, etc. **The following line will start docker container building.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictor = sagemaker_model.deploy(initial_instance_count=1, instance_type='local')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us try to submit a inference job. Here we simply grab two datapoints from the SQuAD dataset and pass the examples to our predictor by calling ```predictor.predict```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## test\n",
    "my_test_example_0 = ('Which NFL team represented the AFC at Super Bowl 50?',\n",
    " 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.')\n",
    "\n",
    "my_test_example_1 = ('Where did Super Bowl 50 take place?',\n",
    " 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.')\n",
    "\n",
    "my_test_examples = (my_test_example_0, my_test_example_1)\n",
    "\n",
    "# mymodel = model_fn(params_path = \"bert_qa-7eb11865.params\")\n",
    "# transform_fn(mymodel, my_test_examples)\n",
    "output = predictor.predict(my_test_examples)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPrediction output: \\n\\n\")\n",
    "\n",
    "for k in output.keys():\n",
    "    print('{}\\n\\n'.format(output[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Up\n",
    "\n",
    "Remove the endpoint after we are done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
